\documentclass[../../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{8}

\begin{document}




\chapter{Advanced Spectral Theory}
\section{Notes}
\begin{itemize}
    \item \marginnote{10/22:}Let $p(z)=\sum_{i=0}^na_iz^i$ be a polynomial. Let $A$ be an $n\times n$ matrix. We let $p(A)=\sum_{i=0}^na_iA^i$.
    \item Theorem: If $A$ is an $n\times n$ and $p(\lambda)=\det(A-\lambda I)$, then $p(A)=0$.
    \begin{itemize}
        \item We know that $p(\lambda)=a(z-\lambda_1)\cdots(z-\lambda_n)$ where $\lambda_1,\dots,\lambda_n$ are the eigenvalues.
        \item Thus $p(A)=a(A-\lambda_1I)\cdots(A-\lambda_nI)$.
        \item If you are in $\R^n$ and have this property, you can factorize your matrix.
        \item Thus, $p(A)\x=\bm{0}$ since $\x$ can be decomposed into a linear combination of eigenvectors of $A$, which will be taken to 0 one by one by the terms of $p(A)$.
    \end{itemize}
    \item $\sigma(B)=\{\text{eigenvalues of }B\}$ is known as the \textbf{spectrum} of $B$.
    \item If $p$ is an arbitrary polynomial and $A$ is $n\times n$, then $\mu$ is an eigenvalue of $p(A)$ if and only if $\mu=p(\lambda)$ where $\lambda$ is an eigenvalue of $A$. In essence, $\sigma(p(A))=p(\sigma(A))$.
    \item Chapter 9 will not be on the exam. We don't have to know the generalization to infinite dimensional spaces.
    \item \marginnote{10/25:}If $A$ is an $n\times n$ square matrix and $p(\lambda)=\det(A-\lambda I)$, then $p(A)=0$.
    \begin{itemize}
        \item Proof: WLOG, let $A$ be an upper triangular matrix with diagonal entries equal to the eigenvalues.
        \item Think of $p(z)=(-1)^n(z-\lambda_1)\cdots(z-\lambda_n)$.
        \item Thus, $p(A)=(-1)^n(A-\lambda_1I)\cdots(A-\lambda_nI)$.
        \item WTS: $p(A)\x=0$ for all $\x\in V$.
        \item Let $E_k=\spn(e_1,\dots,e_k)$ be the span of the first $k$ eigenvectors of $A$, where $e_1,\dots,e_n$ is a standard basis in $\C^n$.
        \item $A$ triangular implies $AE_k\subset E_k$. Thus, $(A-\lambda I)E_k\subset E_k$, so $E_k$ is invariant under $A-\lambda I$ for all $\lambda$.
        \item If we apply $A-\lambda_kI$ to a vector in $E_k$, we are left with a vector in $E_{k-1}$.
        \item Thus, if we apply $\prod_{k=1}^n(A-\lambda_kI)=p(A)$ to any vector in $E_n=V$, we will kill it piece by piece down to zero.
    \end{itemize}
    \item Let $A$ be a square $n\times n$ matrix. Then $p$ an arbitrary polynomial implies $\sigma(p(A))=p(\sigma(A))$. (Any eigenvalue $\mu$ of $p(A)$ is $\mu=p(\lambda)$, where $\lambda$ is an eigenvalue of $A$.)
    \begin{itemize}
        \item Shows that polynomials of operators commute.
        \item Proof: Let $\lambda$ be an eigenvalue of $A$. We want to show that $p(\lambda)$ is an eigenvalue of $p(A)$. This is obvious since $A\x=\lambda\x$ for some $\x$, so $A^k\x=\lambda^k\x$, so in particular, $p(A)\x=p(\lambda)\x$.
        \item On the other hand, if $\mu$ is an eigenvalue of $p(A)$, we want to show that there exists $\lambda\in\sigma(A)$ such that $\mu=p(\lambda)$.
        \item Consider $q(z)=p(z)-\mu$. Then $q(A)=p(A)-\mu I$. Since $\mu$ is an eigenvalue of $p(A)$, $q(A)$ is not invertible.
        \item Thus, $q(z)=(-1)^n(z-z_1)\cdots(z-z_n)$ and $q(A)=(-1)^k(A-z_1I)\cdots(A-z_kI)$.
        \item But $q(A)$ is not invertible, so one of the $A-z_kI$ is not invertible. Take $z_k$ such that $A-z_kI$ is not invertible. Then $z_k\in\sigma(A)$. It follows that $q(z_k)=p(z_k)-\mu=\sigma$.
    \end{itemize}
    \item If $A$ is $n\times n$, $\lambda_1,\dots,\lambda_n$ are its eigenvalues, $p$ is a polynomial, then $p(A)$ is invertible if and only if $p(\lambda_k)\neq 0$ for each $k=1,\dots,n$.
    \begin{itemize}
        \item This is an immediate corollary to the previous result.
    \end{itemize}
    \item We now build up to the \textbf{generalized eigenspace}, which is related to some "geometric" properties of the algebraic multiplicity of an eigenvalue.
    \item If $A:V\to V$ is a linear operator and $E\subset V$ is a subspace, $E$ is $A$-invariant if $AE\subset E$.
    \item Facts:
    \begin{itemize}
        \item If $E$ is $A$-invariant, $E$ is $A^k$-invariant.
        \item Thus, $E$ is $p(A)$-invariant.
    \end{itemize}
    \item Consider the restriction map $A|_E$.
    \item $A$ has a block-diagonalized matrix where each block corresponds to the generalized eigenvectors of a generalized eigenvalue of $A$.
    \begin{itemize}
        \item Let $E_1,\dots,E_r$ be a \textbf{basis of invariant subspaces}.
        \item Let $A_k=A|_{E_k}$. Then the $A_k$'s act independently of each other.
    \end{itemize}
    \item \textbf{Generalized eigenvector} (of $A$): A vector $\vm$ corresponding to an eigenvalue $\lambda$ if there exists $k\geq 1$ such that $(A-\lambda I)^k\vm=\bm{0}$.
    \item \textbf{Generalized eigenspace}: The set $E_\lambda$ of all of the generalized eigenvectors of $\lambda$. \emph{Given by}
    \begin{equation*}
        E_k = \bigcup_{k\geq 1}\ker(A-\lambda I)^k
    \end{equation*}
    \begin{itemize}
        \item $E_\lambda$ is a linear subspace of $V$.
    \end{itemize}
    \item \textbf{Degree} (of $\lambda$): The smallest number $k$ such that increasing $k$ any more does not add further vectors to the generalized eigenspace. \emph{Denoted by} $\bm{d(\lambda)}$. \emph{Also known as} \textbf{depth}.
    \begin{itemize}
        \item Symbolically, $d(\lambda)$ is the smallest number such that
        \begin{equation*}
            E_\lambda = \bigcup_{k=1}^{d(\lambda)}\ker(A-\lambda I)^k
        \end{equation*}
    \end{itemize}
    \item Start working through the first 25 problems of \textcite{bib:Rudin} (his metric spaces problems).
    \item \marginnote{10/27:}Jordan form.
    \item Reviews build up to generalized eigenvectors.
    \item Theorem: If $\sigma(A)=\{\lambda_1,\dots,\lambda_n\}$ and $E_1,\dots,E_n$ are the corresponding generalized eigenspaces, then $E_1,\dots,E_n$ is a basis of subspaces of $U$, i.e., $V=\oplus_kE_k$.
    \item Corollary: $A:V\to V$ can be represented as $A=D+N$ where $D$ is diagonalizable and $N$ is nilpotent and $ND=DN$.
    \begin{itemize}
        \item Proof: Consider the basis of generalized eigenspaces known to exist from the theorem. Then $A=\diag\{A_1,\dots,A_r\}$.
        \item Let
        \begin{equation*}
            N_k = A_k-\lambda_kI_{E_k}
        \end{equation*}
        This is nilpotent.
        \item Then let
        \begin{equation*}
            D = \diag\{\lambda_1I_{E_1},\dots,\lambda_nI_{E_n}\}
        \end{equation*}
        \item These two matrices satisfy the necessary properties.
    \end{itemize}
    \item Let $\dot{\x}=A\x$.
    \begin{itemize}
        \item Let $\x(t)=\e[tA]$, where
        \begin{equation*}
            \e[tA] = \sum\frac{(tA)^k}{k!}
        \end{equation*}
        \item $\norm{\e[tA]}\leq\sum\frac{\norm{A^k}}{k!}=\sum\frac{\norm{A}^k}{k!}$.
        \item Let $p$ be a polynomial of degree $k$. Then
        \begin{equation*}
            p(a+x) = \sum_{k=0}^d\frac{p^{(k)}(a)}{k!}x^k
        \end{equation*}
        \item If $A=D+N$, then...
    \end{itemize}
    \item Nilpotent operators:
    \begin{itemize}
        \item Let $A=\diag\{A_1,\dots,A_r\}$.
        \item We know that $A_k=\lambda_kI_{E_k}+N_k$ for each $k$.
        \item Every nilpotent $N$ can be written in the form
        \begin{equation*}
            \begin{pmatrix}
                0 & 1 &  & 0\\
                 & \ddots & \ddots & \\
                 &  & \ddots & 1\\
                0 &  &  & 0\\
            \end{pmatrix}
        \end{equation*}
    \end{itemize}
    \item \marginnote{10/28:}The exam is long but not that hard. The only question is will you do good or very good.
    \item Revise the previous two homeworks, especially the last two.
    \item No justification for any of the true/false questions. Just circle T or F.
    \begin{itemize}
        \item There are four problems. One is T/F (with multiple subparts); the other three are problem problems (with subparts, too).
        \item Some of the questions will take you 2 seconds. Some you've already seen in the PSets.
        \item The exam is supposed to be boring.
    \end{itemize}
    \item Calculators?
    \begin{itemize}
        \item No calculators needed. Calculators are for chem/physics exams.
        \item Not a lot of computation.
    \end{itemize}
    \item 50 minutes.
    \item Chloe will be proctoring.
    \item Remember the determinant of "special" matrices.
    \begin{itemize}
        \item $|\det U|=1$ if $U$ is unitary.
        \item $\det A=\pm 1$ if $A$ is orthogonal.
        \item Make a list of matrix types that are automatically diagonalizable.
        \item Determinant is the product of the eigenvalues.
        \item Determinant of $A$ is equal to the conjugate of the determinant of $A^*$.
    \end{itemize}
    \item Most of the exercises use the inner product.
    \begin{itemize}
        \item Whenever you had something to prove about eigenvalues or eigenbasis, you went through diagonalization or SVD or the inner product or polar decomposition.
        \item Proving eigenvalues of self-adjoint matrices are real w/ the inner product.
    \end{itemize}
    \item Eigenvalues/eigenvectors of a projection.
    \begin{itemize}
        \item It's implied that it's asking you the multiplicities!!!
    \end{itemize}
    \item Know useful facts but have an idea how to prove them as well.
    \item Recommends against shorthanding in the exams.
    \item Not grading on clarity (since the exam is long).
    \item Max and min are for when you're sure something will be attained. Otherwise use sup and inf.
\end{itemize}



\section{Chapter 9: Advanced Spectral Theory}
\begin{itemize}
    \item \marginnote{10/28:}Theorem 9.1.1 (Cayley-Hamilton): If $p$ is the characteristic polynomial of $A$, $p(A)=0$.
    \item Theorem 9.2.1 (Spectral Mapping Theorem): For a square matrix $A$ and an arbitrary polynomial $p$, $\sigma(p(A))=p(\sigma(A))$. In other words, $\mu$ is an eigenvalue of $p(A)$ if and only if $\mu=p(\lambda)$ for some eigenvalue $\lambda$ of $A$.
    \item Corollary 9.2.2: Let $A$ be a square matrix with eigenvalues $\lambda_1,\dots,\lambda_n$ and let $p$ be a polynomial. Then $p(A)$ is invertible iff $p(\lambda_k)\neq 0$ for all $k=1,\dots,n$.
    \item Algebraic multiplicity is the dimension of the corresponding generalized eigenspace.
\end{itemize}




\end{document}