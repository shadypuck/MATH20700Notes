\documentclass[../../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{6}

\begin{document}




\chapter{Bilinear and Quadratic Forms}
\section{Notes}
\begin{itemize}
    \item \marginnote{10/18:}\textbf{Bilinear form}: A function $L:\R^n\times\R^n\to\R$ such that
    \begin{align*}
        L(\alpha\x_1+\beta\x_2,\y) &= \alpha L(\x_1,\y)+\beta L(\x_2,\y)&
        L(\x,\alpha\y_1+\beta\y_2) &= \alpha L(\x,\y_1)+\beta L(\x,\y_2)
    \end{align*}
    \begin{itemize}
        \item $L(\x,\y)=(A\x,\y)$.
    \end{itemize}
    \item \textbf{Quadratic form}: A bilinear form $L(\x,\x)$.
    \begin{itemize}
        \item $(\x,\x)$ is a polynomial of degree 2 in $\x_1,\dots,\x_n$:
        \begin{equation*}
            L(\lambda\x,\lambda\x) = (\lambda\x,\lambda\x) = \lambda^2(\x,\x)
        \end{equation*}
    \end{itemize}
    \item We have that
    \begin{equation*}
        (A\x,\x) = (A\lambda\x,\lambda\x) = \lambda^2(A\x,\x) = \sum_{j,i=1}^n\alpha_{j,i}\x_i\x_j
    \end{equation*}
    \item The general form of a quadratic form:
    \begin{itemize}
        \item Can any quadratic form on $\R^n$ be written as $(A\x,\x)$?
    \end{itemize}
    \item \marginnote{10/20:}Bilinear forms are linear in each argument when keeping the other fixed.
    \item Quadratic forms $Q(\x)=L(\x,\x)$ are quadratic polynomials in the coordinates of $x$.
    \begin{itemize}
        \item In particular, $Q(\lambda\x)=|\lambda|^2Q(\x)$.
    \end{itemize}
    \item If $Q$ quadratic is real, then $Q(\x)=(A\x,\x)$ where $A$ is some square matrix.
    \begin{itemize}
        \item If $\eb_1,\dots,\eb_n$ is an orthonormal basis of $\R^n$, then there exists a unique $A=A^*$ such that $(A)_{ij}=L(\eb_i,\eb_j)$.
        \item Keeping $\x=\sum_{i=1}^n\x_i,\eb_i$ foxed, we have
        \begin{align*}
            Q(\x) &= L(\x,\x)\\
            &= L(\sum^n\x_i\eb_i,\sum^n\x_j\eb_j)\\
            &= \sum_{i=1}^n\x_iL(\eb_i,\sum^n\x_j\eb_j)\\
            &= \sum_{i,j=1}^n\x_i\x_j\underbrace{L(\eb_i,\eb_j)}_{A_{ij}}
        \end{align*}
    \end{itemize}
    \item We have that
    \begin{align*}
        (A\x,\x) &= (UDU^{-1}\x,\x)\\
        &= (DU^{-1}\x,U^{-1}\x)\\
        &= \sum_{i=1}^n\lambda_i(\underbrace{U^{-1}\x}_{\y_i})_i(\underbrace{U^{-1}\x}_{\y_i})_i
    \end{align*}
    \item Can we characterize the set $\{\x:(A\x,\x)=1\}$?
    \begin{itemize}
        \item Note that this set is equivalent to $\{\y:(D\y,\y)=1\}$ by teh above. This set is a rotation of the previous one. Ellipse?
    \end{itemize}
    \item Positive quadratic form:
    \begin{itemize}
        \item $Q$ is positive definite if $Q(\x)>\bm{0}$ for all $\x\neq\bm{0}$ and $Q$ is positive semidefinite if $Q(\x)\geq\bm{0}$ for all $\x\neq\bm{0}$.
        \item Take a self-adjoint matrix $A=A^*$. It is positive definite if $Q(\x)=(A\x,\x)$ is positive definite.
    \end{itemize}
    \item Theorem: If $A=A^*$, then
    \begin{enumerate}
        \item $A$ is positive definite if and only if all eigenvalues of $A$ are positive.
        \item $A$ is positive semidefinite if and only if all eigenvalues of $A$ are nonnegative.
        \item $A$ is negative semidefinite if and only if all eigenvalues of $A$ are nonpositive.
        \item $A$ is negative definite if and only if all eigenvalues of $A$ are negative.
        \item $A$ is indefinite if and only if the eigenvalues of $A$ have positive and negative values.
    \end{enumerate}
    \item Theorem: $A=A^*$ is positive definite iff $\det A_k>0$ for all $k=1,\dots,n$ where $A_k$ is the upper left $k\times k$ submatrix.
    \item Minimax representation of eigenvalues of a self-adjoint $A$.
    \begin{itemize}
        \item Let $E$ be a subspace of $X$ where $\dim X<\infty$. We define $\codim(E)=\dim E^\perp$.
        \item Thus, $\dim E+\codim E=\dim X$.
        \item Theorem: Let $A=A^*$, $\lambda_1\geq\cdots\geq\lambda_n$ eigenvalues of $A$. Then
        \begin{equation*}
            \lambda_k = \max_{\substack{\text{E subspace}\\\dim E=k}}\min_{\substack{\x\in E\\\norm{\x}=1}}(A\x,\x)
            = \min_{\substack{\text{F subspace}\\\codim F=k-1}}\max_{\substack{\x\in F\\\norm{\x}=1}}(A\x,\x)
        \end{equation*}
        \begin{itemize}
            \item Proof: $A$ diagonal equals $(\lambda_1,\dots,\lambda_n)$.
            \item An orthonormal basis of $X$ such that $\dim E=k$, $\codim F=k-1$, $\dim F=n-k+1$.
            \item There exists an $\x_0\neq\bm{0}$ such that $\x_0\in E\cap F$.
            \item Note that if $B=B^*$, then the max and min of $(B\x,\x)$ over the unit sphere is the maximal and minimal eigenvalue of $B$.
            \item Thus,
            \begin{equation*}
                \min_{\substack{\x\in E\\\norm{\x}=1}}(A\x,\x) \leq (A\x_0,\x_0)
                \leq \max_{\substack{\x\in F\\\norm{\x}=1}}(A\x,\x)
            \end{equation*}
            \item This is true for any $E,F$ subspaces. $\dim E=k$, $\codim F=k-1$, $E_0=\spn(\eb_1,\dots,\eb_k)$ and $F_0=\spn(\eb_k,\dots,\eb_n)$.
            \item Thus,
            \begin{equation*}
                \min_{\substack{E_0\\\norm{\x}=1}}(A\x,\x) = \lambda_k
                = \max_{\substack{F_0\\\norm{\x}=1}}(A\x,\x)
            \end{equation*}
            \item Additionally,
            \begin{equation*}
                \lambda_{k_1} \leq \max{\substack{E\\\dim E=k}}\min_\x(A\x,\x)
                \leq \min_{\substack{F\\\codim F=k-1}}\max_\x(A\x,\x)
                \leq \lambda_k
            \end{equation*}
        \end{itemize}
    \end{itemize}
    \item Corollary: Let $A=A^*=(a_{jk})_{1\leq j,k\leq n}$ with eigenvalues $\lambda_1,\dots,\lambda_n$ listed in decreasing order. Let $\tilde{A}=(a_{j,k})_{1\leq j,k\leq n-1}$ with eigenvalues $\mu_1,\dots,\mu_{n-1}$ listed in decreasing order. Then $\lambda_1\geq\mu_1\geq\lambda_2\geq\mu_2\geq\cdots\geq\mu_{n-1}\geq\lambda_n$.
    \begin{itemize}
        \item Consider $(A\x,\x)$ on $\{\eb_1,\dots,\eb_n\}$, but then restrict yourself to $\x\in\R^{n-1}$ on $\{\eb_1,\dots,\eb_{n-1}\}$.
    \end{itemize}
\end{itemize}



\section{Chapter 7: Bilinear and Quadratic Forms}
\emph{From \textcite{bib:Treil}.}
\begin{itemize}
    \item \marginnote{10/25:}\textbf{Bilinear form} (on $\R^n$): A function $L(\x,\y)$ of two arguments $\x,\y\in\R^n$ that is linear in each argument.
    \begin{itemize}
        \item Linearity in each argument:
        \begin{align*}
            L(\alpha\x_1+\beta\x_2,\y) &= \alpha L(\x_1,\y)+\beta L(\x_2,\y)&
            L(\x,\alpha\y_1+\beta\y_2) &= \alpha L(\x,\y_1)+\beta L(\x,\y_2)
        \end{align*}
    \end{itemize}
    \item If $\x=(x_1,\dots,x_n)^T$ and $\y=(y_1,\dots,y_n)^T$, then
    \begin{align*}
        L(\x,\y) &= \sum_{j,k=1}^na_{j,k}x_ky_j\\
        &= (A\x,\y)\\
        &= \y^TA\x
    \end{align*}
    where
    \begin{equation*}
        A =
        \begin{pmatrix}
            a_{1,1} & \cdots & a_{1,n}\\
            \vdots &  & \vdots\\
            a_{n,1} & \cdots & a_{n,n}\\
        \end{pmatrix}
    \end{equation*}
    \begin{itemize}
        \item $A$ is uniquely determined by $L$.
    \end{itemize}
    \item \textbf{Quadratic form} (on $\R^n$): The diagonal of a bilinear form $L$, i.e., a bilinear form $Q[\x]=L(\x,\x)=(A\x,\x)$.
    \begin{itemize}
        \item Alternatively: A homogeneous polynomial of degree 2, i.e., a polynomial in $x_1,\dots,x_n$ with only $ax_k^2$ and $cx_jx_k$ terms.
    \end{itemize}
    \item There are infinitely many ways to write a quadratic form as $(A\x,\x)$.
    \begin{itemize}
        \item However, there is a unique representation $(A\x,\x)$ where $A$ is a (real) symmetric matrix.
    \end{itemize}
    \item \textbf{Quadratic form} (on $\C^n$): A function of the form $Q[\x]=(A\x,\x)$ where $A$ is self-adjoint.
    \item Lemma 7.1.1: Let $(A\x,\x)$ be real for all $\x\in\C^n$. Then $A=A^*$.
    \item To classify quadratic forms, consider the set of points $\x\in\R^n$ defined by $Q[\x]=1$ for some quadratic form $Q$.
    \begin{itemize}
        \item If the matrix of $Q$ is diagonal, i.e., $Q[\x]=a_1x_1^2+\cdots+a_nx_n^2$, then the set of points can easily be visualized.
    \end{itemize}
    \item The standard method of diagonalizing a quadratic form is change of variables.
    \item Orthogonal diagonalization.
    \begin{itemize}
        \item Let $Q[\x]=(A\x,\x)$ in $\F^n$.
        \item Suppose $\y=S^{-1}\x$ where $S$ is an invertible $n\times n$ matrix. Then
        \begin{equation*}
            Q[\x] = Q[S\y] = (AS\y,S\y) = (S^*AS\y,\y)
        \end{equation*}
        so in the new variables $\y$, the quadratic form has matrix $S^*AS$.
        \item Thus, we can let $A=UDU^*$, choose $D=U^*AU$ as our new (diagonal) matrix, and let this matrix act on the variables $\y=U^*\x$.
        % \item Example:
        % \begin{itemize}
        %     \item Consider $Q[\x]=2x_1^2+2x_2^2+2x_1x_2$ in $\R^2$. We want to describe the set of all $\x\in\R^2$ defined by $Q[\x]=1$.
        %     \item The matrix of $Q$ is
        %     \begin{equation*}
        %         A =
        %         \begin{pmatrix}
        %             2 & 1\\
        %             1 & 2\\
        %         \end{pmatrix}
        %     \end{equation*}
        %     \item Orthogonally diagonalizing gives
        %     \begin{equation*}
        %         D =
        %         \begin{pmatrix}
        %             3 & 0\\
        %             0 & 1\\
        %         \end{pmatrix}
        %     \end{equation*}
        % \end{itemize}
    \end{itemize}
    \item Non-orthogonal diagonalization:
    \begin{itemize}
        \item Completing the square:
        \begin{itemize}
            \item Eliminate all $x_ix_j$ terms by completing the square. Then substitute in a $y_k$ for each squared term.
        \end{itemize}
        \item Row/column operations:
        \begin{itemize}
            \item Augment $(A|I)$. Row reduce $A$ to $D$. Then $I\to S^*$.
        \end{itemize}
    \end{itemize}
    \item \marginnote{10/28:}\textbf{Silvester's Law of Inertia}: For a Hermitian matrix $A$ (i.e., for a quadratic form $Q[\x]=(A\x,\x)$) and any of its diagonalizations $D=S^*AS$, the number of positive, negative, and zero diagonal entries of $D$ depends only on $A$, but not on a particular choice of diagonalization.
    \item \textbf{Positive} (subspace $E\subset\F^n$ corresponding to $A$): A subspace $E$ such that $(A\x,\x)>0$ for all nonzero $\x\in E$. \emph{Also known as} \textbf{$\bm{A}$-positive}.
    \item \textbf{Negative} (subspace $E\subset\F^n$ corresponding to $A$): A subspace $E$ such that $(A\x,\x)<0$ for all nonzero $\x\in E$. \emph{Also known as} \textbf{$\bm{A}$-negative}.
    \item \textbf{Neutral} (subspace $E\subset\F^n$ corresponding to $A$): A subspace $E$ such that $(A\x,\x)=0$ for all nonzero $\x\in E$. \emph{Also known as} \textbf{$\bm{A}$-neutral}.
    \item Theorem 7.3.1: Let $A$ be an $n\times n$ Hermitian matrix, and let $D=S^*AS$ be its diagonalization by an invertible matrix $S$. Then the number of positive (resp. negative) diagonal entries of $D$ coincides with the maximal dimension of an $A$-positive (resp. A-negative) subspace.
    \item Lemma 7.3.2: Let $D=\diag\{\lambda_1,\dots,\lambda_n\}$. Then the number of positive (resp. negative) diagonal entries of $D$ coincides with the maximal dimension of a D-positive (resp. D-negative) subspace.
    \item \textbf{Positive definite} (quadratic form $Q$): A quadratic form $Q$ such that $Q[\x]>0$ for all $\x\neq\bm{0}$.
    \item \textbf{Positive semidefinite} (quadratic form $Q$): A quadratic form $Q$ such that $Q[\x]\geq 0$ for all $\x$.
    \item \textbf{Negative definite} (quadratic form $Q$): A quadratic form $Q$ such that $Q[\x]<0$ for all $\x\neq\bm{0}$.
    \item \textbf{Negative semidefinite} (quadratic form $Q$): A quadratic form $Q$ such that $Q[\x]\leq 0$ for all $\x$.
    \item \textbf{Indefinite} (quadratic form $Q$): A quadratic form $Q$ for which there exist $\x_1,\x_2$ such that $Q[\x_1]>0$ and $Q[\x_2]<0$.
    \item \textbf{Positive definite} (Hermitian matrix $A$): A matrix $A$ for which the corresponding quadratic form $Q[\x]=(A\x,\x)$ is positive definite.
    \begin{itemize}
        \item Positive semidefinite, negative definite, negative semidefinite, and indefinite Hermitian matrices are defined similarly.
    \end{itemize}
    \item Theorem 7.4.1: Let $A=A^*$. Then
    \begin{enumerate}
        \item $A$ is positive definite iff all eigenvalues of $A$ are positive.
        \item $A$ is positive semidefinite iff all eigenvalues of $A$ are non-negative.
        \item $A$ is negative definite iff all eigenvalues of $A$ are negative.
        \item $A$ is negative semidefinite iff all eigenvalues of $A$ are non-positive.
        \item $A$ is indefinite iff it has both positive and negative eigenvalues.
    \end{enumerate}
    \item \textbf{Upper left submatrix} (of $A$): A $k\times k$ matrix $A_k$ composed of all entries of $A$ from row (column) 1 through $k$ in the same arrangement.
    \item Theorem 7.4.2 (Silvester's Criterion of Positivity): A matrix $A=A^*$ is positive definite if and only if $\det A_k>0$ for all $k=1,\dots,n$.
    \begin{itemize}
        \item To check if a matrix $A$ is negative definite, check that the matrix $-A$ is positive definite.
    \end{itemize}
    \item Theorem 7.4.3 (Minimax characterization of eigenvalues): Let $A=A^*$ be an $n\times n$ matrix and let $\lambda_1\geq\cdots\geq\lambda_n$ be its eigenvalues taken in decreasing order. Then
    \begin{equation*}
        \lambda_k = \max_{E:\dim E=k}\min_{\x\in E:\norm{\x}=1}(A\x,\x)
        = \min_{F:\codim F=k-1}\max_{\x\in F:\norm{\x}=1}(A\x,\x)
    \end{equation*}
    \item Corollary 7.4.4 (Intertwining of eigenvalues): Let $A=A^*=\{a_{j,k}\}_{j,k=1}^n$ be a self-adjoint matrix and let $\tilde{A}=\{a_{j,k}\}_{j,k=1}^{n-1}$ be its submatrix of size $(n-1)\times(n-1)$. Let $\lambda_1,\dots,\lambda_n$ and $\mu_1,\dots,\mu_{n-1}$ be the eigenvalues of $A$ and $\tilde{A}$ respectively, taken in decreasing order. Then
    \begin{equation*}
        \lambda_1 \geq \mu_1 \geq \lambda_2 \geq \mu_2 \geq \cdots \geq \lambda_{n-1} \geq \mu_{n-1} \geq \lambda_n
    \end{equation*}
\end{itemize}




\end{document}