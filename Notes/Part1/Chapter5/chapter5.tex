\documentclass[../../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{4}

\begin{document}




\chapter{Inner Product Spaces}
\section{Notes}
\begin{itemize}
    \item \marginnote{10/6:}We define
    \begin{equation*}
        \ell^2(\R) = \left\{ \{a_n\}_{n\geq 1}\subset\R:\sum_1^\infty|a_n|^2<\infty \right\}
    \end{equation*}
    \item \textbf{Inner product}: A map $V\times V\to\F$ that takes $(\x,\y)\mapsto\x\cdot\y$. \emph{Denoted by} $\cdot,(\cdot,\cdot),\langle\cdot,\cdot\rangle$.
    \item Properties of the inner product:
    \begin{itemize}
        \item $(\x,\y)=\overline{(\y,\x)}$ (symmetry).
        \item $(\alpha\x+\beta\y,\z)=\alpha(\x,\z)+\beta(\y,\z)$ (linearity).
        \item $(\x,\x)\geq 0$.
        \item $(\x,\x)=0$ iff $\x=0$.
    \end{itemize}
    \item If $\x,\y\in\R^n$, then
    \begin{equation*}
        (\x,\y) = \sum_{i=1}^nx_iy_i
    \end{equation*}
    \item If $\x,\y\in\C^n$, then
    \begin{equation*}
        (\x,\y) = \sum_{i=1}^nx_i\bar{y}_i
    \end{equation*}
    \item If $f,g\in\Pm_n(t)$, then
    \begin{equation*}
        (f,g) = \int_{-1}^1f\bar{g}\dd{t}
    \end{equation*}
    \begin{itemize}
        \item The conjugate of a polynomial is the polynomial with the conjugate of the coefficients of the original polynomial. Symbolically, if $f=\sum_{i=0}^n\alpha_it^i$ is a polynomial, then $\bar{f}=\sum_{i=0}^n\bar{\alpha}_it^i$.
    \end{itemize}
    \item It is a fact that
    \begin{equation*}
        \left| \sum^\infty a_n\bar{b}_n \right| \leq \norm{(a_n)_{n\geq 1}}\norm{(b_n)_{n\geq 1}}
    \end{equation*}
    \item Suppose we want to define the inner product between two matrices.
    \begin{itemize}
        \item A common one is
        \begin{equation*}
            (A,B) = \trace(B^*A)
        \end{equation*}
        where $B^*=\bar{B}^T=\overline{B^T}$ is the conjugate transpose.
    \end{itemize}
    \item We define the norm as a function $V\to[0,\infty)$ given by
    \begin{equation*}
        \norm{\x} = \sqrt{(\x,\x)}
    \end{equation*}
    \item Properties of the norm.
    \begin{itemize}
        \item $\norm{\alpha\x}=|\alpha|\norm{\x}$.
        \item $\norm{\x+\y}\leq\norm{\x}+\norm{\y}$.
        \item $\norm{\x}=0$ iff $\x=0$.
    \end{itemize}
    \item In $\R^n$,
    \begin{figure}[h!]
        \centering
        \begin{tikzpicture}
            \footnotesize
            \path (-6,0) -- (6,0);
            \draw [->] (-1.5,0) -- (1.5,0);
            \draw [->] (0,-1.5) -- (0,1.5);
    
            \draw [rex,thick] (1,0) -- (0,1) -- (-1,0) -- (0,-1) -- cycle;
            \draw [orx,thick] circle (1cm);
            \draw [grx,thick] (1,1) -- (-1,1) -- (-1,-1) -- (1,-1) -- cycle;
    
            \begin{scope}[xshift=2.5cm,yshift=0.5cm]
                \node [right] at (0,0.8) {${\color{grx}\blacksquare}\ p=\infty$};
                \node [right] at (0,0.4) {${\color{orx}\blacksquare}\ p=2$};
                \node [right] at (0,0) {${\color{rex}\blacksquare}\ p=1$};
            \end{scope}
        \end{tikzpicture}
        \caption{The unit ball of norms corresponding to $p=1,2,\infty$.}
        \label{fig:normUnitBall}
    \end{figure}
    \begin{itemize}
        \item The standard norm is
        \begin{equation*}
            \norm{\x} = \sqrt{\sum|x_i|^2}
        \end{equation*}
        \item We can also define
        \begin{equation*}
            \norm{\x}_p = \sqrt[p]{\sum|x_i|^p}
        \end{equation*}
        \item We can even define
        \begin{equation*}
            \norm{\x}_\infty = \max|x_i|
        \end{equation*}
        \item And we can prove that all of these are valid norms.
        \item Only the norm corresponding to $\ell^2$ is given by an inner product, but all the other quantities are still norms as defined by the properties (see \textcite{bib:Treil}).
        \item Figure \ref{fig:normUnitBall} shows the unit ball of each norm, i.e., the set of all points which have norm 1.
    \end{itemize}
    \item The parallelogram rule:
    \begin{equation*}
        \norm{\x+\y}^2+\norm{\x-\y}^2 = 2(\norm{\x}^2+\norm{\y}^2)
    \end{equation*}
    \item Orthogonality: Given $\vm,\wm$, if $\vm\perp\wm$, then $(\vm,\wm)=0$.
    \item In particular, if $\vm\perp\wm$, then
    \begin{equation*}
        \norm{\vm+\wm}^2 = \norm{\vm}^2+\norm{\wm}^2
    \end{equation*}
    \item Let $E$ be a subspace of $V$. If $\vm\perp E$, then $\vm\perp\eb$ for all $\eb\in E$, i.e., $\vm\perp\text{a set of vectors spanning }E$.
    \item Any set of orthogonal vectors is linearly independent. Thus, if $V$ is $n$ dimensional, then $\vm_1,\dots,\vm_n$ orthogonal is a basis.
    \item Let $E$ be a subspace of $V$. Take $\vm\in V$. We want to define the projection $P_E\vm$ of $\vm$ onto $E$.
    \begin{itemize}
        \item We have that $P_E\vm\in E$ and $v-P_E\vm\perp E$.
        \item Additionally, we have that
        \begin{equation*}
            \norm{\vm-P_E\vm} \leq \norm{\vm-\eb}
        \end{equation*}
        for all $\eb\in E$.
        \item Lastly, we have that $P_E\vm$ is unique.
    \end{itemize}
    \item If we receive a basis of a vector space, how do we create out of that a basis that is orthogonal? The process of doing this is called \textbf{Gram-Schmidt orthogonalization}.
    \begin{itemize}
        \item We keep $\vm_1$, subtract $P_{\vm_1}\vm_2$ from $\vm_2$, subtract $P_{\{\vm_1,\vm_2\}}\vm_3$ from $\vm_3$, and on and on.
    \end{itemize}
    \item If we are given a set of orthogonal vectors, we can normalize them by dividing each by its norm. This creates an orthonormal list. The standard basis is orthonormal.
    \item Let
    \begin{equation*}
        E^\perp = \{v\in V:v\perp E\}
    \end{equation*}
    \begin{itemize}
        \item It follows that $V=E\oplus E^\perp$.
    \end{itemize}
    \item How close can we come to solving $A\x=\bb$ if we cannot solve it exactly (i.e., if the columns are not linearly independent)?
    \begin{itemize}
        \item Let $A$ be an $m\times n$ matrix, and let $\bb\in\R^m$.
        \item Then the best solution is given by minimizing $\norm{A\x-\bb}$. We minimize this with projections. A special case of this is least squares regression! More details in \textcite{bib:Treil}.
    \end{itemize}
    \item \marginnote{10/8:}Soug is gonna send us a hefty amount of reading for the weekend.
    \item Least square approximation:
    \begin{itemize}
        \item If we want to minimize $\norm{A\x-\bb}$, the best we can do is project $\bb$ onto the range of $A$.
        \item Let $\vm_1,\dots,\vm_k$ be an orthogonal basis of $\range A$.
        \item Then
        \begin{equation*}
            \text{Proj}|_{\range A}\bb = \sum^k\frac{(\bb,\vm_k)}{\norm{v_k}^2}\vm_k
        \end{equation*}
        \item Matrix equation form:
        \begin{equation*}
            \text{Projection}_{\range A} = A(A^*A)^{-1}A^*
        \end{equation*}
        if $A^*A$ is invertible, where $A^*=\bar{A}^T$.
        \begin{itemize}
            \item Soug never uses this though.
        \end{itemize}
        \item The minimum is found when $\bb-A\x\perp\range A$. Implies that $\bb-A\x\perp\ab_k$ for all $k$. Implies $(\bb-A\x,\ab_k)={\bar{\ab}_k}^T(\bb-A\x)=0$.
        \item Note that we're letting $\bar{\ab}_k^T$ be the row vector
        \begin{equation*}
            \bar{\ab}_k^T =
            \begin{pmatrix}
                \bar{a}_{1,k} & \cdots & \bar{a}_{n,k}
            \end{pmatrix}
        \end{equation*}
        \item We also have $\bar{A}^T(\bb-A\x)=0$, from which it follows that $A^*A\x=A^*\bb$, so $\x=(A^*A)^{-1}A^*\bb$. Thus, $\text{Proj}|_{\range A}=Ax$, so $\text{Proj}|_{\range A}=A(A^*A)^{-1}A^*\bb$.
    \end{itemize}
    \item Adjoint of a linear map $T:V\to W$ is the $A^*$ discussed above.
    \begin{itemize}
        \item First, we'll do this for matrices. And then we'll do it for any finite-dimensional vector space.
        \item Let $A$ be an $m\times n$ matrix. We claim then that
        \begin{equation*}
            (A\x,\y) = (\x,A^*\y)
        \end{equation*}
        for all $\x\in\C^n$, $\y\in\C^m$. Proof:
        \begin{align*}
            (A\x,\y) &= \bar{\y}^TA\x\\
            &= \y^*A\x\\
            &= (A^*\y)^*\x\\
            &= (\x,A^*\y)
        \end{align*}
        \item Properties of the adjoint:
        \begin{gather*}
            (AB)^T = B^TA^T\\
            (AB)^* = B^*A^*\\
            (A^*)^* = A
        \end{gather*}
        \item $A^*$ is the unique matrix $B$ such that $(A\x,\y)=(\x,B\y)$.
        \item Let $\vm_1,\dots,\vm_n$ be a basis of $V$, and let $\wm_1,\dots,\wm_m$ be a basis of $W$.
        \item Definition of $A^*$: If $(A\x,\y)=(y,A^*\x)$ for all $\x\in V$ and $\y\in W$.
        \item But it's not enough to define something; we have to check that it exists.
        \item If $[A]_{\mathcal{A}\mathcal{B}}$, then $[A^*]_{\mathcal{A}\mathcal{B}}$.
        \item More properties (give criteria for solving systems of equations):
        \begin{gather*}
            \ker A^* = (\range A)^\perp\\
            \ker A = (\range A^*)^\perp\\
            \range A = (\ker A^*)^\perp\\
            \range A^* = (\ker A)^\perp
        \end{gather*}
        \begin{itemize}
            \item Soug proves these.
        \end{itemize}
    \end{itemize}
    \item Isometries and unitary operators.
    \begin{itemize}
        \item $U:X\to Y$ is an isometry if $\norm{\x}=\norm{U\x}$ for all $\x\in X$. It is an isometry because it preserves the distance between points.
        \item It immediately follows that $\norm{\x_1-\x_2}=\norm{U\x_1-U\x_2}=\norm{U(\x_1-\x_2)}$.
        \item This definition is equivalent to an inner product one: $(\x,\y)=(U\x,U\y)$. This follows from the definition of the norm.
        \item We have
        \begin{equation*}
            (\ab,\bb) = \frac{1}{4}\sum_{\alpha=\pm 1,\pm i}\alpha\norm{\ab+\alpha\bb}^2
        \end{equation*}
        \begin{itemize}
            \item $(a+b)^2-(a-b)^2=4ab$ for any $a,b\in\R$, so $ab=\frac{1}{4}[(a+b)^2-(a-b)^2]$. Thus, in a real inner product space,
            \begin{equation*}
                (\ab,\bb) = \frac{1}{4}\left( \norm{\ab+\bb}^2-\norm{\ab-\bb}^2 \right)
            \end{equation*}
            \item It follows that isometries preserve inner products.
        \end{itemize}
        \item $U$ is an isometry if and only if $U^*U=I$. Proof:
        \begin{gather*}
            (\x,\x) = (U\x,U\y) = (U^*U\x,\x)\\
            (\x,\y) = (U\x,U\y) = (U^*U\x,\y)\\
            (\x,\y) = (U\x,U\y) = (\x,\y)
        \end{gather*}
        for all $\y$.
        \item An isometry is unitary if it is invertible.
        \begin{itemize}
            \item Thus, $U:X\to Y$ an isometry is unitary iff $\dim X=\dim Y$.
        \end{itemize}
        \item Note that it follows that $U^*=U^{-1}$ for $U$ an isometry.
        \item $U$ unitary implies $|\det U|=1$, so $\lambda$ an eigenvalue of $U$ implies that $|\lambda|=1$.
        \item $A$ is diagonalizable iff it has an orthogonal basis of eigenvectors.
    \end{itemize}
\end{itemize}



\section{Chapter 5: Inner Product Spaces}
\emph{From \textcite{bib:Treil}.}
\begin{itemize}
    \item \marginnote{10/24:}\textbf{Standard inner product} (on $\C^n$): The inner product $(\z,\wm)$ defined by
    \begin{equation*}
        (\z,\wm) = \wm^*\z
    \end{equation*}
    \item Corollary 5.1.5: Let $\x,\y$ be vectors in an inner product space $V$. The equality $\x=\y$ holds if and only if
    \begin{equation*}
        (\x,\z) = (\y,\z)
    \end{equation*}
    for all $\z\in V$.
    \item Corollary 5.1.6: Suppose two operator $A,B:X\to Y$ satisfy
    \begin{equation*}
        (A\x,\y) = (B\x,\y)
    \end{equation*}
    for all $\x\in x$ and $\y\in Y$. Then $A=B$.
    \item \textbf{Normed space}: A vector space $V$ equipped with a norm that satisfies properties of homogeneity, the triangle inequality, non-negativity, and non-degeneracy.
    \item Any inner product space is naturally a normed space.
    \item If $1\leq p<\infty$, we can define a corresponding norm on $\R^n$ or $\C^n$ by
    \begin{equation*}
        \norm{\x}_p = \left( \sum_{k=1}^n|x_k|^p \right)^{1/p}
    \end{equation*}
    \item We can also define the norm for $p=\infty$ by
    \begin{equation*}
        \norm{\x}_\infty = \max\{|x_k|:k=1,\dots,n\}
    \end{equation*}
    \begin{itemize}
        \item Note that the norm of this form for $p=2$ is the usual norm.
        \item These norms are heavily associated with Figure \ref{fig:normUnitBall}.
    \end{itemize}
    \item \textbf{Minkowski inequality}: One of the triangle inequalities for norms with $p\neq 2$.
    \item Theorem 5.1.11: A norm in a normed space is obtained from some inner product if and only if it satisfies the Parallelogram Identity
    \begin{equation*}
        \norm{\um+\vm}^2+\norm{\um-\vm}^2 = 2(\norm{\um}^2+\norm{\vm}^2)
    \end{equation*}
    for all $\um,\vm\in V$.
    \begin{itemize}
        \item It follows that norms with $p\neq 2$ do not have associated inner products, since such norms fail to satisfy the parallelogram identity.
    \end{itemize}
    \item Lemma 5.2.5 (Generalized Pythagorean Identity): Let $\vm_1,\dots,\vm_n$ be an orthogonal system. Then
    \begin{equation*}
        \norm{\sum_{k=1}^n\alpha_k\vm_k}^2 = \sum_{k=1}^n|\alpha_k|^2\norm{\vm_k}^2
    \end{equation*}
    \item Proposition 5.3.3: Let $\vm_1,\dots,\vm_r$ be an orthogonal basis in $E$. Then the orthogonal projection $P_E\vm$ of a vector $\vm$ is given by the formula
    \begin{equation*}
        P_E\vm = \sum_{k=1}^r\frac{(\vm,\vm_k)}{\norm{\vm_k}^2}\vm_k
    \end{equation*}
    \begin{itemize}
        \item It follows that
        \begin{align*}
            P_E\vm &= \sum_{k=1}^r\frac{\vm_k^*\vm}{\norm{\vm_k}^2}\vm_k\\
            &= \sum_{k=1}^r\frac{1}{\norm{\vm_k}^2}\vm_k\vm_k^*\vm\\
            &= \left( \sum_{k=1}^r\frac{1}{\norm{\vm_k}^2}\vm_k\vm_k^* \right)\vm
        \end{align*}
        \item Thus, we have that
        \begin{equation*}
            P_E = \sum_{k=1}^r\frac{1}{\norm{\vm_k}^2}\vm_k\vm_k^*
        \end{equation*}
    \end{itemize}
    \item \textbf{Gram-Schmidt orthogonalization}: Let $\x_1,\dots,\x_n$ be a linearly independent system of vectors to orthogonalize. Then $\vm_1=\x_1$, $\vm_2=\x_2-P_{\spn\{\vm_1\}}\x_2$, $\vm_3=\x_3-P_{\spn\{\vm_1,\vm_2\}}\x_3$, and on and on.
    \item To find the least squares solution to $A\x=\bb$, solve $A\x=P_{\range A}\bb$.
    \begin{itemize}
        \item We can do this by finding an orthogonal basis of $\range A$ and then applying the projection formula.
        \item Alternatively, we can use the following formula to speed things up if $A^*A$ is invertible:
        \begin{equation*}
            P_{\range A}\bb = A(A^*A)^{-1}A^*\bb
        \end{equation*}
    \end{itemize}
    \item Theorem 5.4.1: For an $m\times n$ matrix $A$,
    \begin{equation*}
        \ker A = \ker(A^*A)
    \end{equation*}
    \begin{itemize}
        \item Thus, $A^*A$ is invertible iff $A$ is invertible iff $A$ is full rank. This gives us a condition on when we can use the projection formula.
    \end{itemize}
    \item Theorem 5.6.1: An operator $U:X\to Y$ is an isometry if and only if it preserves the inner product, i.e., if and only if
    \begin{equation*}
        (\x,\y) = (U\x,U\y)
    \end{equation*}
    for all $\x,\y\in X$.
    \item Lemma 5.6.2: An operator $U:X\to Y$ is an isometry if and only if $U^*U=I$.
    \item \textbf{Unitary} (operator): An invertible isometry.
    \item Proposition 5.6.3: An isometry $U:X\to Y$ is a unitary operator iff $\dim X=\dim Y$.
    \item \textbf{Orthogonal} (matrix): A unitary matrix with real entries.
    \item Unitary operator properties:
    \begin{enumerate}
        \item $U^{-1}=U^*$.
        \item $U$ unitary implies $U^*=U^{-1}$ unitary.
        \item If $\vm_1,\dots,\vm_n$ is orthonormal, $U\vm_1,\dots,U\vm_n$ is orthonormal.
        \item $U,V$ unitary implies $UV$ unitary.
    \end{enumerate}
    \item A matrix $U$ is an isometry iff its columns form an orthonormal system.
    \item Proposition 5.6.4: Let $U$ be a unitary matrix. Then
    \begin{enumerate}
        \item $|\det U|=1$. In particular, if $U$ is orthogonal, then $\det U=\pm 1$.
        \item $|\lambda|=1$ for every eigenvalue $\lambda$ of $U$.
    \end{enumerate}
    \item Proposition 5.6.5: A matrix $A$ is unitarily equivalent to a diagonal one iff it has an orthogonal (orthonormal) basis of eigenvectors.
\end{itemize}




\end{document}