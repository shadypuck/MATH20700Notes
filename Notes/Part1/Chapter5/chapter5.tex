\documentclass[../../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{4}

\begin{document}




\chapter{Inner Product Spaces}
\begin{itemize}
    \item \marginnote{10/6:}We define
    \begin{equation*}
        \ell^2(\R) = \left\{ \{a_n\}_{n\geq 1}\subset\R:\sum_1^\infty|a_n|^2<\infty \right\}
    \end{equation*}
    \item \textbf{Inner product}: A map $V\times V\to\F$ that takes $(\x,\y)\mapsto\x\cdot\y$. \emph{Denoted by} $\cdot,(\cdot,\cdot),\langle\cdot,\cdot\rangle$.
    \item Properties of the inner product:
    \begin{itemize}
        \item $(\x,\y)=\overline{(\y,\x)}$ (symmetry).
        \item $(\alpha\x+\beta\y,\z)=\alpha(\x,\z)+\beta(\y,\z)$ (linearity).
        \item $(\x,\x)\geq 0$.
        \item $(\x,\x)=0$ iff $\x=0$.
    \end{itemize}
    \item If $\x,\y\in\R^n$, then
    \begin{equation*}
        (\x,\y) = \sum_{i=1}^nx_iy_i
    \end{equation*}
    \item If $\x,\y\in\C^n$, then
    \begin{equation*}
        (\x,\y) = \sum_{i=1}^nx_i\bar{y}_i
    \end{equation*}
    \item If $f,g\in\Pm_n(t)$, then
    \begin{equation*}
        (f,g) = \int_{-1}^1f\bar{g}\dd{t}
    \end{equation*}
    \begin{itemize}
        \item The conjugate of a polynomial is the polynomial with the conjugate of the coefficients of the original polynomial. Symbolically, if $f=\sum_{i=0}^n\alpha_it^i$ is a polynomial, then $\bar{f}=\sum_{i=0}^n\bar{\alpha}_it^i$.
    \end{itemize}
    \item It is a fact that
    \begin{equation*}
        \left| \sum^\infty a_n\bar{b}_n \right| \leq \norm{(a_n)_{n\geq 1}}\norm{(b_n)_{n\geq 1}}
    \end{equation*}
    \item Suppose we want to define the inner product between two matrices.
    \begin{itemize}
        \item A common one is
        \begin{equation*}
            (A,B) = \trace(B^*A)
        \end{equation*}
        where $B^*=\bar{B}^T=\overline{B^T}$ is the conjugate transpose.
    \end{itemize}
    \item We define the norm as a function $V\to[0,\infty)$ given by
    \begin{equation*}
        \norm{\x} = \sqrt{(\x,\x)}
    \end{equation*}
    \item Properties of the norm.
    \begin{itemize}
        \item $\norm{\alpha\x}=|\alpha|\norm{\x}$.
        \item $\norm{\x+\y}\leq\norm{\x}+\norm{\y}$.
        \item $\norm{\x}=0$ iff $\x=0$.
    \end{itemize}
    \item In $\R^n$,
    \begin{figure}[h!]
        \centering
        \begin{tikzpicture}
            \footnotesize
            \path (-6,0) -- (6,0);
            \draw [->] (-1.5,0) -- (1.5,0);
            \draw [->] (0,-1.5) -- (0,1.5);
    
            \draw [rex,thick] (1,0) -- (0,1) -- (-1,0) -- (0,-1) -- cycle;
            \draw [orx,thick] circle (1cm);
            \draw [grx,thick] (1,1) -- (-1,1) -- (-1,-1) -- (1,-1) -- cycle;
    
            \begin{scope}[xshift=2.5cm,yshift=0.5cm]
                \node [right] at (0,0.8) {${\color{grx}\blacksquare}\ p=\infty$};
                \node [right] at (0,0.4) {${\color{orx}\blacksquare}\ p=2$};
                \node [right] at (0,0) {${\color{rex}\blacksquare}\ p=1$};
            \end{scope}
        \end{tikzpicture}
        \caption{The unit ball of norms corresponding to $p=1,2,\infty$.}
        \label{fig:normUnitBall}
    \end{figure}
    \begin{itemize}
        \item The standard norm is
        \begin{equation*}
            \norm{\x} = \sqrt{\sum|x_i|^2}
        \end{equation*}
        \item We can also define
        \begin{equation*}
            \norm{\x}_p = \sqrt[p]{\sum|x_i|^p}
        \end{equation*}
        \item We can even define
        \begin{equation*}
            \norm{\x}_\infty = \max|x_i|
        \end{equation*}
        \item And we can prove that all of these are valid norms.
        \item Only the norm corresponding to $\ell^2$ is given by an inner product, but all the other quantities are still norms as defined by the properties (see \textcite{bib:Treil}).
        \item Figure \ref{fig:normUnitBall} shows the unit ball of each norm, i.e., the set of all points which have norm 1.
    \end{itemize}
    \item The parallelogram rule:
    \begin{equation*}
        \norm{\x+\y}^2+\norm{\x-\y}^2 = 2(\norm{\x}^2+\norm{\y}^2)
    \end{equation*}
    \item Orthogonality: Given $\vm,\wm$, if $\vm\perp\wm$, then $(\vm,\wm)=0$.
    \item In particular, if $\vm\perp\wm$, then
    \begin{equation*}
        \norm{\vm+\wm}^2 = \norm{\vm}^2+\norm{\wm}^2
    \end{equation*}
    \item Let $E$ be a subspace of $V$. If $\vm\perp E$, then $\vm\perp\eb$ for all $\eb\in E$, i.e., $\vm\perp\text{a set of vectors spanning }E$.
    \item Any set of orthogonal vectors is linearly independent. Thus, if $V$ is $n$ dimensional, then $\vm_1,\dots,\vm_n$ orthogonal is a basis.
    \item Let $E$ be a subspace of $V$. Take $\vm\in V$. We want to define the projection $P_E\vm$ of $\vm$ onto $E$.
    \begin{itemize}
        \item We have that $P_E\vm\in E$ and $v-P_E\vm\perp E$.
        \item Additionally, we have that
        \begin{equation*}
            \norm{\vm-P_E\vm} \leq \norm{\vm-\eb}
        \end{equation*}
        for all $\eb\in E$.
        \item Lastly, we have that $P_E\vm$ is unique.
    \end{itemize}
    \item If we receive a basis of a vector space, how do we create out of that a basis that is orthogonal? The process of doing this is called \textbf{Gram-Schmidt orthogonalization}.
    \begin{itemize}
        \item We keep $\vm_1$, subtract $P_{\vm_1}\vm_2$ from $\vm_2$, subtract $P_{\{\vm_1,\vm_2\}}\vm_3$ from $\vm_3$, and on and on.
    \end{itemize}
    \item If we are given a set of orthogonal vectors, we can normalize them by dividing each by its norm. This creates an orthonormal list. The standard basis is orthonormal.
    \item Let
    \begin{equation*}
        E^\perp = \{v\in V:v\perp E\}
    \end{equation*}
    \begin{itemize}
        \item It follows that $V=E\oplus E^\perp$.
    \end{itemize}
    \item How close can we come to solving $A\x=\bb$ if we cannot solve it exactly (i.e., if the columns are not linearly independent)?
    \begin{itemize}
        \item Let $A$ be an $m\times n$ matrix, and let $\bb\in\R^m$.
        \item Then the best solution is given by minimizing $\norm{A\x-\bb}$. We minimize this with projections. A special case of this is least squares regression! More details in \textcite{bib:Treil}.
    \end{itemize}
    \item \marginnote{10/8:}Soug is gonna send us a hefty amount of reading for the weekend.
    \item Least square approximation:
    \begin{itemize}
        \item If we want to minimize $\norm{A\x-\bb}$, the best we can do is project $\bb$ onto the range of $A$.
        \item Let $\vm_1,\dots,\vm_k$ be an orthogonal basis of $\range A$.
        \item Then
        \begin{equation*}
            \text{Proj}|_{\range A}\bb = \sum^k\frac{(\bb,\vm_k)}{\norm{v_k}^2}\vm_k
        \end{equation*}
        \item Matrix equation form:
        \begin{equation*}
            \text{Projection}_{\range A} = A(A^*A)^{-1}A^*
        \end{equation*}
        if $A^*A$ is invertible, where $A^*=\bar{A}^T$.
        \begin{itemize}
            \item Soug never uses this though.
        \end{itemize}
        \item The minimum is found when $\bb-A\x\perp\range A$. Implies that $\bb-A\x\perp\ab_k$ for all $k$. Implies $(\bb-A\x,\ab_k)={\bar{\ab}_k}^T(\bb-A\x)=0$.
        \item Note that we're letting $\bar{\ab}_k^T$ be the row vector
        \begin{equation*}
            \bar{\ab}_k^T =
            \begin{pmatrix}
                \bar{a}_{1,k} & \cdots & \bar{a}_{n,k}
            \end{pmatrix}
        \end{equation*}
        \item We also have $\bar{A}^T(\bb-A\x)=0$, from which it follows that $A^*A\x=A^*\bb$, so $\x=(A^*A)^{-1}A^*\bb$. Thus, $\text{Proj}|_{\range A}=Ax$, so $\text{Proj}|_{\range A}=A(A^*A)^{-1}A^*\bb$.
    \end{itemize}
    \item Adjoint of a linear map $T:V\to W$ is the $A^*$ discussed above.
    \begin{itemize}
        \item First, we'll do this for matrices. And then we'll do it for any finite-dimensional vector space.
        \item Let $A$ be an $m\times n$ matrix. We claim then that
        \begin{equation*}
            (A\x,\y) = (\x,A^*\y)
        \end{equation*}
        for all $\x\in\C^n$, $\y\in\C^m$. Proof:
        \begin{align*}
            (A\x,\y) &= \bar{\y}^TA\x\\
            &= \y^*A\x\\
            &= (A^*\y)^*\x\\
            &= (\x,A^*\y)
        \end{align*}
        \item Properties of the adjoint:
        \begin{gather*}
            (AB)^T = B^TA^T\\
            (AB)^* = B^*A^*\\
            (A^*)^* = A
        \end{gather*}
        \item $A^*$ is the unique matrix $B$ such that $(A\x,\y)=(\x,B\y)$.
        \item Let $\vm_1,\dots,\vm_n$ be a basis of $V$, and let $\wm_1,\dots,\wm_m$ be a basis of $W$.
        \item Definition of $A^*$: If $(A\x,\y)=(y,A^*\x)$ for all $\x\in V$ and $\y\in W$.
        \item But it's not enough to define something; we have to check that it exists.
        \item If $[A]_{\mathcal{A}\mathcal{B}}$, then $[A^*]_{\mathcal{A}\mathcal{B}}$.
        \item More properties (give criteria for solving systems of equations):
        \begin{gather*}
            \ker A^* = (\range A)^\perp\\
            \ker A = (\range A^*)^\perp\\
            \range A = (\ker A^*)^\perp\\
            \range A^* = (\ker A)^\perp
        \end{gather*}
        \begin{itemize}
            \item Soug proves these.
        \end{itemize}
    \end{itemize}
    \item Isometries and unitary operators.
    \begin{itemize}
        \item $U:X\to Y$ is an isometry if $\norm{\x}=\norm{U\x}$ for all $\x\in X$. It is an isometry because it preserves the distance between points.
        \item It immediately follows that $\norm{\x_1-\x_2}=\norm{U\x_1-U\x_2}=\norm{U(\x_1-\x_2)}$.
        \item This definition is equivalent to an inner product one: $(\x,\y)=(U\x,U\y)$. This follows from the definition of the norm.
        \item We have
        \begin{equation*}
            (\ab,\bb) = \frac{1}{4}\sum_{\alpha=\pm 1,\pm i}\alpha\norm{\ab+\alpha\bb}^2
        \end{equation*}
        \begin{itemize}
            \item $(a+b)^2-(a-b)^2=4ab$ for any $a,b\in\R$, so $ab=\frac{1}{4}[(a+b)^2-(a-b)^2]$. Thus, in a real inner product space,
            \begin{equation*}
                (\ab,\bb) = \frac{1}{4}\left( \norm{\ab+\bb}^2-\norm{\ab-\bb}^2 \right)
            \end{equation*}
            \item It follows that isometries preserve inner products.
        \end{itemize}
        \item $U$ is an isometry if and only if $U^*U=I$. Proof:
        \begin{gather*}
            (\x,\x) = (U\x,U\y) = (U^*U\x,\x)\\
            (\x,\y) = (U\x,U\y) = (U^*U\x,\y)\\
            (\x,\y) = (U\x,U\y) = (\x,\y)
        \end{gather*}
        for all $\y$.
        \item An isometry is unitary if it is invertible.
        \begin{itemize}
            \item Thus, $U:X\to Y$ an isometry is unitary iff $\dim X=\dim Y$.
        \end{itemize}
        \item Note that it follows that $U^*=U^{-1}$ for $U$ an isometry.
        \item $U$ unitary implies $|\det U|=1$, so $\lambda$ an eigenvalue of $U$ implies that $|\lambda|=1$.
        \item $A$ is diagonalizable iff it has an orthogonal basis of eigenvectors.
    \end{itemize}
    \item \marginnote{10/11:}Spectral decomposition of self-adjoint linear maps.
    \begin{itemize}
        \item Can we write a map in term of the eigenvalues only?
        \item Let $A:X\to X$ be linear and self-adjoint. Where $\dim X<\infty$.
        \item Let $A$ have eigenvalues $\lambda_1,\dots,\lambda_n$ and eigenvectors $\vm_1,\dots,\vm_n$. The there is an orthonormal basis of $X$ consisting of eigenvectors of $A$. An operator is self-adjoint if $A=A^*$.
        \item If $A$ is self-adjoint, then $A$ can be written as diagonal with the eigenvalues on the diagonal with respect to some orthonormal basis of eigenvectors.
        \item Let $\F=\C$.
    \end{itemize}
    \item If there exists an orthonormal basis $u_1,\dots,u_n$ of $X$ such that $A$ is triangular, then $A=UTU^*$ where $U$ is unitary and $T$ is upper triangular.
    \begin{itemize}
        \item Proved with induction on $\dim X$.
        \item $\dim X=1$ is clear.
        \item Assume for $\dim X=n-1$, WTS for $\dim X=n$.
        \item The subspace has a basis $\vm_1,\dots,\vm_{n-1}$ such that $A$ has a diagonal form.
        \item Let $u\in X$ be linearly independent of $\vm_1,\dots,\vm_{n-1}$.
        \item Let $\lambda$ be the remaining eigenvalue and $u$ the corresponding eigenvector. Let $E=\spn(u)$. Then make the matrix $\lambda$ in the upper left corner, and block diagonal with "$A_{n-1}$" in the bottom right corner, zeroes everywhere else.
    \end{itemize}
    \item \textbf{Self-adjoint} (matrix $A$): A linear map $A:X\to X$ where $\dim X<\infty$ such that $A=A^*$.
    \begin{itemize}
        \item Similarly, $(Ax,y)=(x,Ay)$.
        \item $A$ self-adjoint implies all eigenvalues are real, eigenvectors corresponding to different eigenvalues are orthogonal.
        \begin{itemize}
            \item Soug proves this.
        \end{itemize}
    \end{itemize}
    \item \textbf{Strictly positive} (operator $A$): A self-adjoint operator $A:X\to X$ such that $(Ax,x)>0$ for all $x\neq 0$. \emph{Also known as} \textbf{positive definite}.
    \begin{itemize}
        \item Implies that all eigenvalues are strictly positive.
    \end{itemize}
    \item \textbf{Nonnegative} (operator $A$): A self-adjoint operator $A:X\to X$ such that $(Ax,x)\geq 0$ for all $x\neq 0$. \emph{Also known as} \textbf{definite}.
    \begin{itemize}
        \item All eigenvalues are nonnegative.
    \end{itemize}
    \item Suppose $A\geq 0$ is self-adjoint. Then there exists a unique self-adjoint $B\geq 0$ such that $B^2=A$.
    \begin{itemize}
        \item $A$ self-adjoint is diagonal (wrt. some basis).
        \item $A$ positive means that all eigenvalues (diagonal entries) are positive.
        \item Thus, take
        \begin{equation*}
            B =
            \begin{pmatrix}
                \sqrt{\lambda_1} & 0 & 0\\
                0 & \ddots & 0\\
                0 & 0 & \sqrt{\lambda_n}\\
            \end{pmatrix}
        \end{equation*}
        \item Suppose $B^2=A$, $C^2=A$. Then we have an orthonormal basis corresponding to $B$ and an orthonormal basis corresponding to $C$. It follows that $B^2=C^2=A$. Write $B^2x$ and $C^2x$ in terms of their bases; will necessitate that the bases are the same.
    \end{itemize}
\end{itemize}




\end{document}