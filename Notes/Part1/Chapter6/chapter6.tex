\documentclass[../../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{5}

\begin{document}




\chapter{Structure of Operators on Inner Product Spaces}
\section{Notes}
\begin{itemize}
    \item \marginnote{10/11:}Spectral decomposition of self-adjoint linear maps.
    \begin{itemize}
        \item Can we write a map in term of the eigenvalues only?
        \item Let $A:X\to X$ be linear and self-adjoint. Where $\dim X<\infty$.
        \item Let $A$ have eigenvalues $\lambda_1,\dots,\lambda_n$ and eigenvectors $\vm_1,\dots,\vm_n$. The there is an orthonormal basis of $X$ consisting of eigenvectors of $A$. An operator is self-adjoint if $A=A^*$.
        \item If $A$ is self-adjoint, then $A$ can be written as diagonal with the eigenvalues on the diagonal with respect to some orthonormal basis of eigenvectors.
        \item Let $\F=\C$.
    \end{itemize}
    \item If there exists an orthonormal basis $u_1,\dots,u_n$ of $X$ such that $A$ is triangular, then $A=UTU^*$ where $U$ is unitary and $T$ is upper triangular.
    \begin{itemize}
        \item Proved with induction on $\dim X$.
        \item $\dim X=1$ is clear.
        \item Assume for $\dim X=n-1$, WTS for $\dim X=n$.
        \item The subspace has a basis $\vm_1,\dots,\vm_{n-1}$ such that $A$ has a diagonal form.
        \item Let $u\in X$ be linearly independent of $\vm_1,\dots,\vm_{n-1}$.
        \item Let $\lambda$ be the remaining eigenvalue and $u$ the corresponding eigenvector. Let $E=\spn(u)$. Then make the matrix $\lambda$ in the upper left corner, and block diagonal with "$A_{n-1}$" in the bottom right corner, zeroes everywhere else.
    \end{itemize}
    \item \textbf{Self-adjoint} (matrix $A$): A linear map $A:X\to X$ where $\dim X<\infty$ such that $A=A^*$.
    \begin{itemize}
        \item Similarly, $(Ax,y)=(x,Ay)$.
        \item $A$ self-adjoint implies all eigenvalues are real, eigenvectors corresponding to different eigenvalues are orthogonal.
        \begin{itemize}
            \item Soug proves this.
        \end{itemize}
    \end{itemize}
    \item \textbf{Strictly positive} (operator $A$): A self-adjoint operator $A:X\to X$ such that $(Ax,x)>0$ for all $x\neq 0$. \emph{Also known as} \textbf{positive definite}.
    \begin{itemize}
        \item Implies that all eigenvalues are strictly positive.
    \end{itemize}
    \item \textbf{Nonnegative} (operator $A$): A self-adjoint operator $A:X\to X$ such that $(Ax,x)\geq 0$ for all $x\neq 0$. \emph{Also known as} \textbf{definite}.
    \begin{itemize}
        \item All eigenvalues are nonnegative.
    \end{itemize}
    \item Suppose $A\geq 0$ is self-adjoint. Then there exists a unique self-adjoint $B\geq 0$ such that $B^2=A$.
    \begin{itemize}
        \item $A$ self-adjoint is diagonal (wrt. some basis).
        \item $A$ positive means that all eigenvalues (diagonal entries) are positive.
        \item Thus, take
        \begin{equation*}
            B =
            \begin{pmatrix}
                \sqrt{\lambda_1} & 0 & 0\\
                0 & \ddots & 0\\
                0 & 0 & \sqrt{\lambda_n}\\
            \end{pmatrix}
        \end{equation*}
        \item Suppose $B^2=A$, $C^2=A$. Then we have an orthonormal basis corresponding to $B$ and an orthonormal basis corresponding to $C$. It follows that $B^2=C^2=A$. Write $B^2x$ and $C^2x$ in terms of their bases; will necessitate that the bases are the same.
    \end{itemize}
    \item \marginnote{10/13:}If we get yes/no questions, we don't have to justify.
    \item Cauchy-Schwarz inequality:
    \begin{equation*}
        |(\x,\y)| \leq \norm{\x}\norm{\y}
    \end{equation*}
    \begin{itemize}
        \item Real spaces, $V$ vs. $(\cdot,\cdot)$ inner product.
        \item Proof:
        \begin{align*}
            0 &\leq \norm{\x+t\y}^2\\
            &= t^2\norm{\y^2}+2t(\x,\y)+\norm{\x}^2
        \end{align*}
        Thus, the discriminant must be less than zero (because the whole polynomial is positive, so the discriminant [the opposite of the $x^0$ term of the factored form of the polynomial] must be less than zero so the polynomial doesn't get dragged down to negative values):
        \begin{equation*}
            (\x,\y)^2-\norm{\x}^2\norm{\y}^2 \leq 0
        \end{equation*}
        Taking square roots of both sides proves the desired inequality.
    \end{itemize}
    \item Recall that if $A^*=A$, then all eigenvalues are real and all eigenvectors of distinct eigenvalues are orthogonal to each other.
    \item \textbf{Normal} (matrix): A matrix $N$ such that $N^*N=NN^*$.
    \begin{itemize}
        \item Examples: Diagonal, self-adjoint, and unitary operators are all normal.
    \end{itemize}
    \item Any normal operator in a complex vector spae has an orthonormal set of eigenvectors, e.g., $N=UDU^*$.
    \begin{itemize}
        \item Proof: $N$ is upper triangular wrt. some basis (because all matrices are). WTS any normal upper triangular matrix is diagonal. Done by induction on the dimension of $N$ from $n=2$.
        \item Assume the claim for every $(n-1)\times(n-1)$ normal upper triangular matrix.
        \item Let
        \begin{equation*}
            N =
            \begin{pmatrix}
                a_{11} & a_{12} & \cdots & a_{1n}\\
                0 &  &  & \\
                0 &  &  & \\
                \vdots &  & N_1 & \\
                0 &  &  & \
            \end{pmatrix}
        \end{equation*}
        (we know every normal matrix can be written in this upper triangular form)
        \item Then just compute $NN^*$ and $N^*N$. Knowing they have to be equal, we have that $a_{12}=\cdots=a_{1n}=0$.
        \item We can also prove from the above (block diagonal multiplication) that $N_1$ is normal. Thus, it's diagonal, too. Therefore, the whole thing is diagonal.
    \end{itemize}
    \item $N$ is normal if and only if $\norm{N\x}=\norm{N^*\x}$.
    \begin{itemize}
        \item Proof: $(N\x,N\y)=(N^*N\x,\y)=(NN^*\x,\y)=(N^*\x,N^*\y)$. This is equivalent to the desired condition.
    \end{itemize}
    \item If $A$ is nonnegative and $(A\eb_k,\eb_k)=a_{kk}$, then
    \begin{equation*}
        \sum_{i,j=1}^na_{ij}\x_i\x_j
    \end{equation*}
    \item \textbf{Positive definite} (matrix): An $n\times n$ self-adjoint matrix such that $(A\x,\x)>0$ for all $\x\in X$.
    \item Let $A:X\to Y$, $\dim X=\dim Y$. Then $AA^*$ is positive semidefinite. And there exists a unique square root $R=\sqrt{A^*A}$.
    \begin{itemize}
        \item Proof: $(A^*A\x,\x)=(A\x,A\x)=\norm{A\x}^2\geq 0$.
    \end{itemize}
    \item \textbf{Modulus} (of $A$): The matrix $|A|=\sqrt{A^*A}$.
    \item Check $\norm{|A|\x}=\norm{A\x}$.
    \begin{equation*}
        \norm{|A|\x}^2
        = (|A|\x,|A|\x)
        = (|A|^*|A|\x,\x)
        = (A^*A\x,\x)
        = (A\x,A\x)
        = \norm{A\x}^2
    \end{equation*}
    \item Let $A:X\to X$ be a linear operator. Then $A=U|A|$ where $U$ is unitary.
    \item Look at singular matrices.
    \item \marginnote{10/15:}Recall that if $A:X\to Y$, we have that $A^*A$ is semidefinite, positive, and self adjoint.
    \begin{itemize}
        \item Thus, there exists a unique matrix $R=\sqrt{A^*A}\geq 0$, which we define to be $|A|=\sqrt{A^*A}$.
    \end{itemize}
    \item Polar form of a matrix:
    \begin{equation*}
        A = U|A|
    \end{equation*}
    \begin{itemize}
        \item This may not be unique!
        \item Proof: Suppose $A\x=U(|A|\x)$. $A\x\in\range A$, and $|A|\x\in\range(|A|)$. $\x\in\range(|A|)$ implies that there exists $\vm\in X$ such that $x=|A|\vm$.
        \item Define $U\x=A\x$. $U$ is a well-defined linear map.
        \item $\norm{U_0\x}=\norm{A\x}=\norm{|A|\vm}=\norm{\x}$.
        \item $U$ is an isometry.
        \item $\range|A|\to X$.
        \item Use $\ker A=\ker|A|=(\range A)^\perp$ to extend $U_0$ to $U$: $U=U_0+U_1$.
    \end{itemize}
    \item \textbf{Singular values} (of a matrix): The eigenvalues of $|A|$.
    \begin{itemize}
        \item So if $\lambda_1,\dots,\lambda_n$ are the eigenvalues of $A^*A$, the singular values of $A$ are $\sqrt{\lambda_1},\dots,\sqrt{\lambda_n}$.
    \end{itemize}
    \item Let $A:X\to Y$ be a linear map.
    \begin{itemize}
        \item Let $\sigma_1,\dots,\sigma_n$ be the signular values of $A$. Then $\sigma_1,\dots,\sigma_n>0$.
        \item Additionally, if $\vm_1,\dots,\vm_n$ is an orthonormal basis of eigenvectors of $A^*A$, then the list of $n$ vectors $\wm_1,\dots,\wm_n$ defined by $\wm_k=1/\sigma_kA\vm_k$ for each $k=1,\dots,n$ is orthonormal.
        \begin{itemize}
            \item Proof:
            \begin{equation*}
                (\wm_k,\wm_j) = \frac{1}{\sigma_k\sigma_k}(A\vm_k,A\vm_j)
                = \frac{1}{\sigma_k\sigma_j}
                = \frac{1}{\sigma_k\sigma_j}(A^*A\vm_k,\vm_j)
                = \frac{\sigma_k^2}{\sigma_k\sigma_j}(\vm_k,\vm_j)
                = 0
            \end{equation*}
            and
            \begin{equation*}
                \norm{\wm_k} = \frac{1}{\sigma_k}\norm{A\vm_k}
                = \frac{1}{\sigma_k}\norm{|A|\vm_k}
                = 1
            \end{equation*}
        \end{itemize}
        \item Schmidt decomposition of $A$:
        \begin{equation*}
            A\x = \sum^r\sigma_k(\x,\vm_k)\wm_k
        \end{equation*}
        \begin{itemize}
            \item This is because $\x=\sum(\x,\vm_k)\vm_k$, so by the above,
            \begin{equation*}
                A\x = \sum^n(\x,\vm_k)A\vm_k
                = \sum^r\sigma_k(\x,\vm_k)\wm_k
            \end{equation*}
        \end{itemize}
    \end{itemize}
    \item \textbf{Operator norm}: $\norm{A}=\max\{\norm{A\x}:\norm{\x}\leq 1\}$.
    \item Properties of the operator norm:
    \begin{itemize}
        \item $\norm{A\x}\leq\norm{A}\norm{\x}$.
        \item $\norm{\alpha A}=|\alpha|\norm{A}$.
        \item $\norm{A+B}\leq\norm{A}+\norm{B}$.
        \item $\norm{A}\geq 0$.
        \item $\norm{A}=0$ iff $A=0$.
    \end{itemize}
    \item \textbf{Frobenius norm}: The norm $\norm{A}_2^2=\trace(A^*A)$.
    \item The operator norm is always less than or equal to the Frobenius norm.
    \item If $A:\F^n\to\F^n$, then $A=W\Sigma V^*$ where $\sigma$ is a diagonal matrix of nonzero singular values.
    \item The operator norm of $A$ is the largest of the singular values.
    \item An orthogonal matrix can be decomposed to a block-diagonal matrix of rotations.
    \item \marginnote{10/18:}Soug tests what he teaches and doesn't give super tricky questions.
    \item Structure of orthogonal matrices.
    \item \textbf{Orthogonal} (matrix): A unitary matrix $U$ with all elements real and $|\det U|=1$.
    \item Theorem: Let $U$ be an orthogonal operator on $\R^n$ such that $\det U=1$. Then there exists an orthonormal basis $\vm_1,\dots,\vm_n$ such that with respect to this basis,
    \begin{equation*}
        U =
        \begin{pmatrix}
            R_{\phi_1} &  &  & \bm{0}\\
             & \ddots &  & \\
             &  & \R_{\phi_k} & \\
            \bm{0} &  &  & I_{n-2k}\\
        \end{pmatrix}
    \end{equation*}
    where each $R_{\phi_i}$ is a $2\times 2$ rotation matrix.
    \begin{itemize}
        \item If you are in $\R^7$ for example, you would be able to express $U$ as a composition of at most 3 rotation maps and the identity map.
        \item Each rotation map acts on two orthonormal vectors.
        \item Proof: $P(\lambda)$ is the $n$-degree characteristic polynomial $\det(U-\lambda I)=0$. The eigenvalues are the roots of it.
        \item $p(\lambda)=0$ if and only if $p(\bar{\lambda})=0$.
        \begin{itemize}
            \item $\lambda\in\C$ is an eigenvalue with eigenvector $\um\neq 0$ iff $U\um=\lambda\um$ and $U\bar{\um}=\bar{\lambda}\bar{\um}$.
        \end{itemize}
        \item Recall that $U$ unitary implies $|\lambda|=1$.
        \begin{itemize}
            \item Proof\footnote{This would be a good exam question.}: $\norm{U\x}=\norm{\x}$ and $U\x=\lambda\x$. Thus,
            \begin{equation*}
                \norm{U\x} = \norm{\lambda\x} = |\lambda|\norm{\x} = \norm{\x}
            \end{equation*}
            and since $\x\neq 0$, we can divide by $\norm{\x}$, so $|\lambda|=1$.
        \end{itemize}
        \item Let $\um=\Re\um+\Im\um$.
        \item It follows that we may define
        \begin{align*}
            \x &= \Re\um = \frac{\um+\bar{\um}}{2}&
            \y &= \Im\um = \frac{\um-\bar{\um}}{2}
        \end{align*}
        \item Thus, $\um=\x+i\y$ and $\bar{\um}=\x-i\y$.
        \item Since $U\x=\frac{U\um+U\bar{\um}}{2}=\frac{\lambda\um+\bar{\lambda}\bar{\um}}{2}$, $U\y=\Im(\lambda\um)=\Re(\lambda\um)$.
        \item Since $|\lambda|=1$, $\lambda=\e[i\alpha]$ and $\bar{\lambda}=\e[-i\alpha]$.
        \item It follows that $U\x=(\cos\alpha)\x-(\sin\alpha)\y$ and $U\y=(\cos\alpha)\y)+(\sin\alpha)\x$.
        \item Thus, since $U\x=\Re\lambda\um$, we have that
        \begin{align*}
            \lambda\um &= (\cos\alpha+i\sin\alpha)(\x+i\y)\\
            &= (\cos\alpha)\x-(\sin\alpha)\y+i[(\cos\alpha)\y+(\sin\alpha)\x]
        \end{align*}
        \item If $E_\lambda$ is a 2 dimensional space spanned by $\x$ and $\y$ and invariant by $U$. Thus, any block of the desired matrix leaves its desired sub-block invariant.
        \item We also know that the eigenvectors of a unitary matrix corresponding to different eigenvalues are orthogonal.
        \item Thus, $\norm{\x}=\norm{\y}=\sqrt{2}/2\norm{\um}$, $\x\perp\y$.
        \item Let $\x,\y$ complete the theorem to form a basis of $\R^n$.
        \item It will follow that
        \begin{equation*}
            U =
            \begin{pmatrix}
                R_\alpha & \bm{0}\\
                \bm{0} & U_1\\
            \end{pmatrix}
        \end{equation*}
        where $U_1$ is orthogonal, and we may repeat the process.
    \end{itemize}
\end{itemize}



\section{Chapter 6: Structure of Operators on Inner Product Spaces}
\emph{From \textcite{bib:Treil}.}
\begin{itemize}
    \item \marginnote{10/24:}Theorem 6.1.1: Let $A:X\to X$ be an operator acting in a complex inner product space. Then there exists an orthonormal basis $\um_1,\dots,\um_n$ of $X$ such that the matrix of $A$ in this basis is upper triangular. In other words, any $n\times n$ matrix $A$ can be represented as $A=UTU^*$, where $U$ is unitary and $T$ is upper-triangular.
    \item Theorem 6.1.2: Let $A:X\to X$ be an operator acting on a real inner product space. Suppose that all eigenvalues of $A$ are real. Then there exists an orthonormal basis $\um_1,\dots,\um_n$ in $X$ such that the matrix of $A$ in this basis is upper triangular. In other words, any real $n\times n$ matrix $A$ with all real eigenvalues can be represented as $T=UTU^*=UTU^T$, where $U$ is orthogonal and $T$ is a real upper-triangular matrix.
    \item Theorem 6.2.1: Let $A=A^*$ be a self-adjoint operator in an inner product space $X$ (the space can be complex or real). Then all eigenvalues of $A$ are real and there exists an orthonormal basis of eigenvectors of $A$ in $X$.\par
    Equivalently (see Theorem 6.2.2), $A$ can be represented as $A=UDU^*$ where $U$ is a unitary matrix and $D$ is a diagonal matrix with real entries. Moreover, if $A$ is real, $U$ can be chosen to be real, i.e., orthogonal.
    \item Proposition 6.2.3: Let $A=A^*$ be a self-adjoint operator and let $\lambda,\um,\mu,\vm$ be such that $A\um=\lambda\um$ and $A\vm=\mu\vm$. Then if $\lambda\neq\mu$, $\um\perp\vm$.
    \item Since complex multiplication is commutative,
    \begin{equation*}
        D^*D = DD^*
    \end{equation*}
    for every diagonal matrix $D$.
    \begin{itemize}
        \item It follows that $A^*A=AA^*$ if the matrix of $A$ in some orthonormal basis is diagonal.
    \end{itemize}
    \item Theorem 6.2.4: Any normal operator $N$ in a complex vector space has an orthonormal basis of eigenvectors.\par
    Equivalently, any matrix $N$ satisfying $N^*N=NN^*$ can be represented as $N=UDU^*$ where $U$ is unitary and $D$ is diagonal.
    \item Proposition 6.2.5: An operator $N:X\to X$ is normal iff
    \begin{equation*}
        \norm{N\x} = \norm{N^*\x}
    \end{equation*}
    for all $\x\in X$.
    \item \textbf{Hermitian square} (of $A$): The matrix $A^*A$.
    \item \textbf{Modulus} (of $A$): The unique positive semidefinite square root $\sqrt{A^*A}$.
    \item Proposition 6.3.3: For a linear operator $A:X\to Y$,
    \begin{equation*}
        \norm{|A|\x} = \norm{A\x}
    \end{equation*}
    \item Corollary 6.3.4: $\ker A=\ker|A|$.
    \item Theorem 6.3.5: Let $A:X\to X$ be an operator (square matrix). Then $A$ can be represented as
    \begin{equation*}
        A = U|A|
    \end{equation*}
    where $U$ is a unitary operator.
    \item \textbf{Singular value} (of $A$): An eigenvalue of $|A|$.
    \begin{itemize}
        \item A positive square root of an operator of $A^*A$.
    \end{itemize}
    \item Proposition 6.3.6: Let $\sigma_1,\dots,\sigma_n$ be the singular values of $A$, ordered such that $\sigma_1,\dots,\sigma_r$ are the nonzero singular values, and let $\vm_1,\dots,\vm_n$ be an orthonormal basis of eigenvectors of $A^*A$. Then the system
    \begin{equation*}
        \wm_k = \frac{1}{\sigma_k}A\vm_k
    \end{equation*}
    for $k=1,\dots,r$ is orthonormal.
    \item \textbf{Schmidt decomposition} (of $A$): The decompositions
    \begin{equation*}
        A = \sum_{k=1}^r\sigma_k\wm_k\vm_k^*
    \end{equation*}
    and
    \begin{equation*}
        A\x = \sum_{k=1}^r\sigma_k(\x,\vm_k)\wm_k
    \end{equation*}
    \begin{itemize}
        \item Note that these can be verified by plugging $\x=\vm_j$ for each $j=1,\dots,n$ into the latter equation.
    \end{itemize}
    \item \marginnote{10/25:}Lemma 6.3.7: $A$ can be represented as the Schmidt decomposition
    \begin{equation*}
        A = \sum_{k=1}^r\sigma_k\wm_k\vm_k^*
    \end{equation*}
    where $\sigma_k>0$ for any orthonormal systems $\vm_1,\dots,\vm_r$ and $\wm_1,\dots,\wm_r$.
    \item Corollary 6.3.8: Let $A=\sum_{k=1}^r\sigma_k\wm_k\vm_k^*$ be a Schmidt decomposition of $A$. Then
    \begin{equation*}
        A^* = \sum_{k=1}^r\sigma_k\vm_k\wm_k^*
    \end{equation*}
    is a Schmidt decomposition of $A^*$.
    \item \textbf{Reduced singular value decomposition} (of $A$): The decomposition
    \begin{equation*}
        A = \tilde{W}\tilde{\Sigma}\tilde{V}^*
    \end{equation*}
    where $A:\F^n\to\F^m$ has the Schmidt decomposition $A=\sum_{k=1}^r\sigma_k\wm_k\vm_k^*$, $\tilde{\Sigma}=\diag\{\sigma_1,\dots,\sigma_r\}$, and $\tilde{V},\tilde{W}$ are matrices with columns $\vm_1,\dots,\vm_r$ and $\wm_1,\dots,\wm_r$, respectively. \emph{Also known as} \textbf{compact singular value decomposition}.
    \begin{itemize}
        \item Note that $\tilde{V}$ is an $n\times r$ matrix, $\tilde{\Sigma}$ is an $r\times r$ matrix, and $\tilde{W}$ is an $m\times r$ matrix.
        \item Since $\vm_1,\dots,\vm_r$ and $\wm_1,\dots,\wm_r$ are orthonormal, $\tilde{V},\tilde{W}$ are isometries.
    \end{itemize}
    \item Note that $r=\rank A$ (see Problem 6.3.1).
    \begin{itemize}
        \item It follows that if $A$ is invertible, then $m=n=r$, so $\tilde{V},\tilde{W}$ are unitary and $\tilde{\Sigma}$ is an invertible diagonal matrix.
    \end{itemize}
    \item However, $A$ need not be invertible for us to get a representation similar to $A=\tilde{W}\tilde{\Sigma}\tilde{V}^*$.
    \begin{itemize}
        \item Complete $\vm_1,\dots,\vm_r$ and $\wm_1,\dots,\wm_r$ to bases of $\F^n$ and $\F^m$, respectively.
        \item Then we get the following.
    \end{itemize}
    \item \textbf{Singular value decomposition} (of $A$): The decomposition
    \begin{equation*}
        A = W\Sigma V^*
    \end{equation*}
    where $V\in M_{n\times n}^\F$ and $W\in M_{m\times m}^\F$ are unitary matrices with columns $\vm_1,\dots,\vm_n$ and $\wm_1,\dots,\wm_n$, respectively, and $\Sigma\in M_{m\times n}^{\R^+}$ is a "diagonal" matrix such that
    \begin{equation*}
        \Sigma_{j,k} =
        \begin{cases}
            \sigma_k & j=k\leq r\\
            0 & \text{otherwise}
        \end{cases}
    \end{equation*}
    \item Notice that if $A=W\Sigma V^*$, then
    \begin{equation*}
        A^*A = (W\Sigma V^*)^*(W\Sigma V^*) = V\Sigma^*W^*W\Sigma V^* = V\Sigma^2V^*
    \end{equation*}
    proving that the singular values of $A$, squared, are the eigenvalues of $A^*A$.
    \item If $A$ is invertible, the reduced SVD is the matrix form of the Schmidt decomposition is the SVD.
    \item If $A=W\Sigma V^*$ is $n\times n$, then
    \begin{equation*}
        A = (\underbrace{WV^*}_U)(\underbrace{V\Sigma V^*}_{|A|})
    \end{equation*}
    is a polar decomposition of $A$.
    \item Consider the unit ball $B=\{\x\in\R^n:\norm{\x}\leq 1\}$.
    \begin{itemize}
        \item We want to describe $A(B)$, i.e., the image of the unit ball under $A$.
        \item Let $\x=(x_1,\dots,x_n)^T$ and let $\y=(y_1,\dots,y_n)^T$. If $A=\diag\{\sigma_1,\dots,\sigma_n\}$, we have $\y\in A(B)$ iff $\y=A\x$ where $\x\in B$ iff
        \begin{equation*}
            \sum_{k=1}^n\frac{y_k^2}{\sigma_k^2} = \sum_{k=1}^nx_k^2 = \norm{\x}^2 \leq 1
        \end{equation*}
        \item Thus, $A(B)$ is an ellipsoid with half-axes $\sigma_1,\dots,\sigma_n$.
        \item In the more general case, if $A=W\Sigma V^*$, then since $V^*$ is unitary, $V^*(B)=B$. $\Sigma V^*(B)=\Sigma(B)$ is thus by the above an ellipsoid in $\range\Sigma$ with half-axes $\sigma_1,\dots,\sigma_r$. Thus, since isometries don't change geometry, $W(\Sigma(B))$ is also an ellipsoid with the same half-axes, but in $\range A$.
    \end{itemize}
    \item Conclusion: The image $A(B)$ of the closed unit ball $B$ is an ellipsoid in $\range A$ with half-axes $\sigma_1,\dots,\sigma_r$, where $r$ is the number of nonzero singular values, i.e., the rank of $A$.
    \item Finding the maximum of $\norm{A\x}$ for $\x\in B$.
    \begin{itemize}
        \item For a diagonal matrix $\Sigma$ with nonnegative entries, the maximum is clearly the maximal diagonal entry: In this case if $s_1$ is the maximal diagonal entry, then since
        \begin{equation*}
            \Sigma\x = \sum_{k=1}^rs_kx_k\eb_k
        \end{equation*}
        we have that
        \begin{equation*}
            \norm{A\x}^2 = \sum_{k=1}^rs_k^2|x_k|^2
            \leq s_1^2\sum_{k=1}^r|x_k|^2\\
            = s_1^2\cdot\norm{\x}^2
        \end{equation*}
        \item We get the following by a similar logic to before.
    \end{itemize}
    \item Conclusion: The maximum of $\norm{A\x}$ on the unit ball $B$ is the maximal singular value of $A$.
    \item \textbf{Operator norm} (of $A$): The following quantity. \emph{Denoted by} $\norm{\bm{A}}$. \emph{Given by}
    \begin{equation*}
        \norm{A} = \max\{\norm{A\x}:\x\in X,\norm{\x}\leq 1\}
    \end{equation*}
    \begin{itemize}
        \item $\norm{A}$ clearly satisfies the four properties of a norm.
        \item Additionally,
        \begin{equation*}
            \norm{A\x} \leq \norm{A}\cdot\norm{\x}
        \end{equation*}
        \item Alternate definition: The operator norm $\norm{A}$ is the smallest number $C\geq 0$ such that $\norm{A\x}\leq C\norm{\x}$.
    \end{itemize}
    \item \textbf{Frobenius norm}: The following norm. \emph{Also known as} \textbf{Hilbert-Schmidt norm}. \emph{Denoted by} $\norm{\bm{A}}_{\bm{2}}$. \emph{Given by}
    \begin{equation*}
        \norm{A}_2^2 = \trace(A^*A)
    \end{equation*}
    \begin{itemize}
        \item If we let $s_1,\dots,s_n$ be the singular values of $A$ and let $s_1$ be the largest value, then we have
        \begin{equation*}
            \norm{A}^2 = s_1^2
            \leq \sum_{k=1}^ns_k^2
            = \trace(A^*A)
            = \norm{A}_2^2
        \end{equation*}
    \end{itemize}
    \item Conclusion: The operator norm of a matrix cannot be more than its Frobenius norm.
    \item Suppose we want to solve $A\x=\bb$ where $A$ is invertible, but there is some (experimental) error $\Delta\bb$ in $\bb$. Then we are really solving for an approximate solution $\x+\Delta\x$ to the equation
    \begin{equation*}
        A(\x+\Delta\x) = \bb+\Delta\bb
    \end{equation*}
    \begin{itemize}
        \item It follows since $A$ is invertible that $\x=A^{-1}\bb$ and $\Delta\x=A^{-1}\Delta\bb$.
        \item To estimate the relative error $\norm{\Delta\x}/\norm{\x}$ in the solution in comparison with the relative error $\norm{\Delta\bb}/\norm{\bb}$ in the data, use
        \begin{equation*}
            \frac{\norm{\Delta\x}}{\norm{\x}} = \frac{\norm{A^{-1}\Delta\bb}}{\norm{\bb}}\frac{\norm{A\x}}{\norm{\x}}
            \leq \frac{\norm{A^{-1}}\cdot\norm{\Delta\bb}}{\norm{\bb}}\frac{\norm{A}\cdot\norm{\x}}{\norm{\x}}
            = \norm{A^{-1}}\cdot\norm{A}\cdot\frac{\norm{\Delta\bb}}{\norm{\bb}}
        \end{equation*}
    \end{itemize}
    \item \textbf{Condition number} (of $A$): The following quantity. \emph{Given by}
    \begin{equation*}
        \norm{A}\cdot\norm{A^{-1}}
    \end{equation*}
    \begin{itemize}
        \item If $s_1$ is the largest singular value of $A$ and $s_n$ is the smallest, then
        \begin{equation*}
            \norm{A}\cdot\norm{A^{-1}} = s_1\cdot\frac{1}{s_n} = \frac{s_1}{s_n}
        \end{equation*}
    \end{itemize}
    \item \textbf{Well-conditioned} (matrix): A matrix the condition number of which is not "too big."
    \item \textbf{Ill-conditioned} (matrix): A matrix that is not well-conditioned.
    \item Theorem 6.5.1: Let $U$ be an orthogonal operator on $\R^n$ and let $\det U=1$. Then there exists an orthonormal basis $\vm_1,\dots,\vm_n$ such that the matrix of $U$ in this basis has the block diagonal form
    \begin{equation*}
        \begin{pmatrix}
            R_{\varphi_1} &  &  & 0\\
             & \ddots &  & \\
             &  & R_{\varphi_k} & \\
            0 &  &  & I_{n-2k}\\
        \end{pmatrix}
    \end{equation*}
    where each $R_{\varphi_j}$ is a two-dimensional rotation
    \begin{equation*}
        R_{\varphi_j} =
        \begin{pmatrix}
            \cos\varphi_j & -\sin\varphi_j\\
            \sin\varphi_j & \cos\varphi_j\\
        \end{pmatrix}
    \end{equation*}
    and $I_{n-2k}$ represents the $(n-2k)\times(n-2k)$ identity matrix.
    \begin{itemize}
        \item Alternate interpretation: Any rotation in $\R^n$ can be represented as a composition of at most $n/2$ commuting planar rotations.
    \end{itemize}
    \item Theorem 6.5.2: Let $U$ be an orthogonal operator on $\R^n$ and let $\det U=-1$. Then there exists an orthonormal basis $\vm_1,\dots,\vm_n$ such that the matrix of $U$ in this basis has block diagonal form
    \begin{equation*}
        \begin{pmatrix}
            R_{\varphi_1} &  &  &  & 0\\
             & \ddots &  &  & \\
             &  & R_{\varphi_k} &  & \\
             &  &  & I_r & \\
            0 &  &  &  & -1\\
        \end{pmatrix}
    \end{equation*}
    where $r=n-2k-1$ and each $R_{\varphi_j}$ is a two-dimensional rotation
    \begin{equation*}
        R_{\varphi_j} =
        \begin{pmatrix}
            \cos\varphi_j & -\sin\varphi_j\\
            \sin\varphi_j & \cos\varphi_j\\
        \end{pmatrix}
    \end{equation*}
    \item Corollary: An orthogonal $2\times 2$ matrix $U$ with determinant $-1$ is always a reflection.
    \item Theorem 6.5.3: Any rotation $U$ (i.e., any orthogonal transformation $U$ with $\det U=1$) can be represented as a product of at most $n(n-1)/2$ elementary rotations.
    \item Consider the following orthonormal bases of $\R^2$.
    \begin{figure}[h!]
        \centering
        \begin{subfigure}[b]{0.3\linewidth}
            \centering
            \begin{tikzpicture}[scale=2]
                \footnotesize
                \draw [stealth-stealth] (0,1) node[above]{$\eb_2$} -- (0,0) -- (1,0) node[right]{$\eb_1$};
            \end{tikzpicture}
            \caption{}
            \label{fig:orientationR2a}
        \end{subfigure}
        \begin{subfigure}[b]{0.3\linewidth}
            \centering
            \begin{tikzpicture}[scale=2,rotate=30]
                \footnotesize
                \draw [stealth-stealth] (0,1) node[above]{$\vm_2$} -- (0,0) -- (1,0) node[right]{$\vm_1$};
            \end{tikzpicture}
            \caption{}
            \label{fig:orientationR2b}
        \end{subfigure}
        \begin{subfigure}[b]{0.3\linewidth}
            \centering
            \begin{tikzpicture}[scale=2,rotate=-30]
                \footnotesize
                \draw [stealth-stealth] (0,1) node[above]{$\vm_1$} -- (0,0) -- (1,0) node[right]{$\vm_2$};
            \end{tikzpicture}
            \caption{}
            \label{fig:orientationR2c}
        \end{subfigure}
        \caption{Orientation in $\R^2$.}
        \label{fig:orientationR2}
    \end{figure}
    \begin{itemize}
        \item Notice that a rotation will get you from the standard basis (a) to basis (b), but not from the standard basis (a) to basis (c).
        \item This is the motivation for defining orientation.
        \item More formally, we know that there is a unique linear transformation $U$ such that $U\eb_k=\vm_k$ for each $k=1,2$. In particular, the matrix of $U$ with respect to the standard basis is orthogonal with columns $\vm_1,\vm_2$.
        \item By Theorems 6.5.1 and 6.5.2, if $\det U=1$, then $U$ is a rotation, and if $\det U=-1$, then $U$ is not a rotation.
    \end{itemize}
    \item \textbf{Similarly oriented} (bases $\mathcal{A},\mathcal{B}$): Two bases $\mathcal{A},\mathcal{B}$ of a real vector space such that the change of coordinates matrix $[I]_{\mathcal{B}\mathcal{A}}$ has a positive determinant.
    \item \textbf{Differently oriented} (bases $\mathcal{A},\mathcal{B}$): Two bases $\mathcal{A},\mathcal{B}$ of a real vector space that are not similarly oriented (i.e., $[I]_{\mathcal{B}\mathcal{A}}$ has a negative determinant).
    \item We usually let the standard basis of $\R^n$ have a \textbf{positive orientation}.
    \begin{itemize}
        \item In an abstract vector space, we need only fix a basis and declare its orientation to be positive.
    \end{itemize}
    \item \textbf{Continuously transformable} (bases $\mathcal{A},\mathcal{B}$): Two bases $\mathcal{A},\mathcal{B}$ such that $\mathcal{A}=\{\ab_1,\dots,\ab_n\}$ can be continuously transformed to a basis $\mathcal{B}=\{\bb_1,\dots,\bb_n\}$. In particular, there exists a \textbf{continuous family of bases} $\mathcal{V}(t)=\{\vm_1(t),\dots,\vm_n(t)\}$, $t\in[a,b]$, such that
    \begin{align*}
        \vm_k(a) &= \ab_k&
        \vm_k(b) &= \bb_k
    \end{align*}
    for each $k=1,\dots,n$.
    \item \textbf{Continuous family of bases}: A family of bases $\mathcal{V}(t)=\{\vm_1(t),\dots,\vm_n(t)\}$, $t\in[a,b]$, such that the vector-functions $\vm_k(t)$ are continuous (their coordinates in some bases are continuous functions) and the system $\vm_1(t),\dots,\vm_n(t)$ is a basis for all $t\in[a,b]$.
    \item Theorem 6.6.1: Two bases $\mathcal{A}=\{\ab_1,\dots,\ab_n\}$ and $\mathcal{B}=\{\bb_1,\dots,\bb_n\}$ have the same orientation if and only if one of the bases can be continuously transformed to the other.
\end{itemize}




\end{document}