\documentclass[../../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{5}

\begin{document}




\chapter{Structure of Operators on Inner Product Spaces}
\section{Notes}
\begin{itemize}
    \item \marginnote{10/11:}Spectral decomposition of self-adjoint linear maps.
    \begin{itemize}
        \item Can we write a map in term of the eigenvalues only?
        \item Let $A:X\to X$ be linear and self-adjoint. Where $\dim X<\infty$.
        \item Let $A$ have eigenvalues $\lambda_1,\dots,\lambda_n$ and eigenvectors $\vm_1,\dots,\vm_n$. The there is an orthonormal basis of $X$ consisting of eigenvectors of $A$. An operator is self-adjoint if $A=A^*$.
        \item If $A$ is self-adjoint, then $A$ can be written as diagonal with the eigenvalues on the diagonal with respect to some orthonormal basis of eigenvectors.
        \item Let $\F=\C$.
    \end{itemize}
    \item If there exists an orthonormal basis $u_1,\dots,u_n$ of $X$ such that $A$ is triangular, then $A=UTU^*$ where $U$ is unitary and $T$ is upper triangular.
    \begin{itemize}
        \item Proved with induction on $\dim X$.
        \item $\dim X=1$ is clear.
        \item Assume for $\dim X=n-1$, WTS for $\dim X=n$.
        \item The subspace has a basis $\vm_1,\dots,\vm_{n-1}$ such that $A$ has a diagonal form.
        \item Let $u\in X$ be linearly independent of $\vm_1,\dots,\vm_{n-1}$.
        \item Let $\lambda$ be the remaining eigenvalue and $u$ the corresponding eigenvector. Let $E=\spn(u)$. Then make the matrix $\lambda$ in the upper left corner, and block diagonal with "$A_{n-1}$" in the bottom right corner, zeroes everywhere else.
    \end{itemize}
    \item \textbf{Self-adjoint} (matrix $A$): A linear map $A:X\to X$ where $\dim X<\infty$ such that $A=A^*$.
    \begin{itemize}
        \item Similarly, $(Ax,y)=(x,Ay)$.
        \item $A$ self-adjoint implies all eigenvalues are real, eigenvectors corresponding to different eigenvalues are orthogonal.
        \begin{itemize}
            \item Soug proves this.
        \end{itemize}
    \end{itemize}
    \item \textbf{Strictly positive} (operator $A$): A self-adjoint operator $A:X\to X$ such that $(Ax,x)>0$ for all $x\neq 0$. \emph{Also known as} \textbf{positive definite}.
    \begin{itemize}
        \item Implies that all eigenvalues are strictly positive.
    \end{itemize}
    \item \textbf{Nonnegative} (operator $A$): A self-adjoint operator $A:X\to X$ such that $(Ax,x)\geq 0$ for all $x\neq 0$. \emph{Also known as} \textbf{definite}.
    \begin{itemize}
        \item All eigenvalues are nonnegative.
    \end{itemize}
    \item Suppose $A\geq 0$ is self-adjoint. Then there exists a unique self-adjoint $B\geq 0$ such that $B^2=A$.
    \begin{itemize}
        \item $A$ self-adjoint is diagonal (wrt. some basis).
        \item $A$ positive means that all eigenvalues (diagonal entries) are positive.
        \item Thus, take
        \begin{equation*}
            B =
            \begin{pmatrix}
                \sqrt{\lambda_1} & 0 & 0\\
                0 & \ddots & 0\\
                0 & 0 & \sqrt{\lambda_n}\\
            \end{pmatrix}
        \end{equation*}
        \item Suppose $B^2=A$, $C^2=A$. Then we have an orthonormal basis corresponding to $B$ and an orthonormal basis corresponding to $C$. It follows that $B^2=C^2=A$. Write $B^2x$ and $C^2x$ in terms of their bases; will necessitate that the bases are the same.
    \end{itemize}
    \item \marginnote{10/13:}If we get yes/no questions, we don't have to justify.
    \item Cauchy-Schwarz inequality:
    \begin{equation*}
        |(\x,\y)| \leq \norm{\x}\norm{\y}
    \end{equation*}
    \begin{itemize}
        \item Real spaces, $V$ vs. $(\cdot,\cdot)$ inner product.
        \item Proof:
        \begin{align*}
            0 &\leq \norm{\x+t\y}^2\\
            &= t^2\norm{\y^2}+2t(\x,\y)+\norm{\x}^2
        \end{align*}
        Thus, the discriminant must be less than zero (because the whole polynomial is positive, so the discriminant [the opposite of the $x^0$ term of the factored form of the polynomial] must be less than zero so the polynomial doesn't get dragged down to negative values):
        \begin{equation*}
            (\x,\y)^2-\norm{\x}^2\norm{\y}^2 \leq 0
        \end{equation*}
        Taking square roots of both sides proves the desired inequality.
    \end{itemize}
    \item Recall that if $A^*=A$, then all eigenvalues are real and all eigenvectors of distinct eigenvalues are orthogonal to each other.
    \item \textbf{Normal} (matrix): A matrix $N$ such that $N^*N=NN^*$.
    \begin{itemize}
        \item Examples: Diagonal, self-adjoint, and unitary operators are all normal.
    \end{itemize}
    \item Any normal operator in a complex vector spae has an orthonormal set of eigenvectors, e.g., $N=UDU^*$.
    \begin{itemize}
        \item Proof: $N$ is upper triangular wrt. some basis (because all matrices are). WTS any normal upper triangular matrix is diagonal. Done by induction on the dimension of $N$ from $n=2$.
        \item Assume the claim for every $(n-1)\times(n-1)$ normal upper triangular matrix.
        \item Let
        \begin{equation*}
            N =
            \begin{pmatrix}
                a_{11} & a_{12} & \cdots & a_{1n}\\
                0 &  &  & \\
                0 &  &  & \\
                \vdots &  & N_1 & \\
                0 &  &  & \
            \end{pmatrix}
        \end{equation*}
        (we know every normal matrix can be written in this upper triangular form)
        \item Then just compute $NN^*$ and $N^*N$. Knowing they have to be equal, we have that $a_{12}=\cdots=a_{1n}=0$.
        \item We can also prove from the above (block diagonal multiplication) that $N_1$ is normal. Thus, it's diagonal, too. Therefore, the whole thing is diagonal.
    \end{itemize}
    \item $N$ is normal if and only if $\norm{N\x}=\norm{N^*\x}$.
    \begin{itemize}
        \item Proof: $(N\x,N\y)=(N^*N\x,\y)=(NN^*\x,\y)=(N^*\x,N^*\y)$. This is equivalent to the desired condition.
    \end{itemize}
    \item If $A$ is nonnegative and $(A\eb_k,\eb_k)=a_{kk}$, then
    \begin{equation*}
        \sum_{i,j=1}^na_{ij}\x_i\x_j
    \end{equation*}
    \item \textbf{Positive definite} (matrix): An $n\times n$ self-adjoint matrix such that $(A\x,\x)>0$ for all $\x\in X$.
    \item Let $A:X\to Y$, $\dim X=\dim Y$. Then $AA^*$ is positive semidefinite. And there exists a unique square root $R=\sqrt{A^*A}$.
    \begin{itemize}
        \item Proof: $(A^*A\x,\x)=(A\x,A\x)=\norm{A\x}^2\geq 0$.
    \end{itemize}
    \item \textbf{Modulus} (of $A$): The matrix $|A|=\sqrt{A^*A}$.
    \item Check $\norm{|A|\x}=\norm{A\x}$.
    \begin{equation*}
        \norm{|A|\x}^2
        = (|A|\x,|A|\x)
        = (|A|^*|A|\x,\x)
        = (A^*A\x,\x)
        = (A\x,A\x)
        = \norm{A\x}^2
    \end{equation*}
    \item Let $A:X\to X$ be a linear operator. Then $A=U|A|$ where $U$ is unitary.
    \item Look at singular matrices.
    \item \marginnote{10/15:}Recall that if $A:X\to Y$, we have that $A^*A$ is semidefinite, positive, and self adjoint.
    \begin{itemize}
        \item Thus, there exists a unique matrix $R=\sqrt{A^*A}\geq 0$, which we define to be $|A|=\sqrt{A^*A}$.
    \end{itemize}
    \item Polar form of a matrix:
    \begin{equation*}
        A = U|A|
    \end{equation*}
    \begin{itemize}
        \item This may not be unique!
        \item Proof: Suppose $A\x=U(|A|\x)$. $A\x\in\range A$, and $|A|\x\in\range(|A|)$. $\x\in\range(|A|)$ implies that there exists $\vm\in X$ such that $x=|A|\vm$.
        \item Define $U\x=A\x$. $U$ is a well-defined linear map.
        \item $\norm{U_0\x}=\norm{A\x}=\norm{|A|\vm}=\norm{\x}$.
        \item $U$ is an isometry.
        \item $\range|A|\to X$.
        \item Use $\ker A=\ker|A|=(\range A)^\perp$ to extend $U_0$ to $U$: $U=U_0+U_1$.
    \end{itemize}
    \item \textbf{Singular values} (of a matrix): The eigenvalues of $|A|$.
    \begin{itemize}
        \item So if $\lambda_1,\dots,\lambda_n$ are the eigenvalues of $A^*A$, the singular values of $A$ are $\sqrt{\lambda_1},\dots,\sqrt{\lambda_n}$.
    \end{itemize}
    \item Let $A:X\to Y$ be a linear map.
    \begin{itemize}
        \item Let $\sigma_1,\dots,\sigma_n$ be the signular values of $A$. Then $\sigma_1,\dots,\sigma_n>0$.
        \item Additionally, if $\vm_1,\dots,\vm_n$ is an orthonormal basis of eigenvectors of $A^*A$, then the list of $n$ vectors $\wm_1,\dots,\wm_n$ defined by $\wm_k=1/\sigma_kA\vm_k$ for each $k=1,\dots,n$ is orthonormal.
        \begin{itemize}
            \item Proof:
            \begin{equation*}
                (\wm_k,\wm_j) = \frac{1}{\sigma_k\sigma_k}(A\vm_k,A\vm_j)
                = \frac{1}{\sigma_k\sigma_j}
                = \frac{1}{\sigma_k\sigma_j}(A^*A\vm_k,\vm_j)
                = \frac{\sigma_k^2}{\sigma_k\sigma_j}(\vm_k,\vm_j)
                = 0
            \end{equation*}
            and
            \begin{equation*}
                \norm{\wm_k} = \frac{1}{\sigma_k}\norm{A\vm_k}
                = \frac{1}{\sigma_k}\norm{|A|\vm_k}
                = 1
            \end{equation*}
        \end{itemize}
        \item Schmidt decomposition of $A$:
        \begin{equation*}
            A\x = \sum^r\sigma_k(\x,\vm_k)\wm_k
        \end{equation*}
        \begin{itemize}
            \item This is because $\x=\sum(\x,\vm_k)\vm_k$, so by the above,
            \begin{equation*}
                A\x = \sum^n(\x,\vm_k)A\vm_k
                = \sum^r\sigma_k(\x,\vm_k)\wm_k
            \end{equation*}
        \end{itemize}
    \end{itemize}
    \item \textbf{Operator norm}: $\norm{A}=\max\{\norm{A\x}:\norm{\x}\leq 1\}$.
    \item Properties of the operator norm:
    \begin{itemize}
        \item $\norm{A\x}\leq\norm{A}\norm{\x}$.
        \item $\norm{\alpha A}=|\alpha|\norm{A}$.
        \item $\norm{A+B}\leq\norm{A}+\norm{B}$.
        \item $\norm{A}\geq 0$.
        \item $\norm{A}=0$ iff $A=0$.
    \end{itemize}
    \item \textbf{Frobenious norm}: The norm $\norm{A}_2^2=\trace(A^*A)$.
    \item The operator norm is always less than or equal to the Frobenius norm.
    \item If $A:\F^n\to\F^n$, then $A=W\Sigma V^*$ where $\sigma$ is a diagonal matrix of nonzero singular values.
    \item The operator norm of $A$ is the largest of the singular values.
    \item An orthogonal matrix can be decomposed to a block-diagonal matrix of rotations.
    \item \marginnote{10/18:}Soug tests what he teaches and doesn't give super tricky questions.
    \item Structure of orthogonal matrices.
    \item \textbf{Orthogonal} (matrix): A unitary matrix $U$ with all elements real and $|\det U|=1$.
    \item Theorem: Let $U$ be an orthogonal operator on $\R^n$ such that $\det U=1$. Then there exists an orthonormal basis $\vm_1,\dots,\vm_n$ such that with respect to this basis,
    \begin{equation*}
        U =
        \begin{pmatrix}
            R_{\phi_1} &  &  & \bm{0}\\
             & \ddots &  & \\
             &  & \R_{\phi_k} & \\
            \bm{0} &  &  & I_{n-2k}\\
        \end{pmatrix}
    \end{equation*}
    where each $R_{\phi_i}$ is a $2\times 2$ rotation matrix.
    \begin{itemize}
        \item If you are in $\R^7$ for example, you would be able to express $U$ as a composition of at most 3 rotation maps and the identity map.
        \item Each rotation map acts on two orthonormal vectors.
        \item Proof: $P(\lambda)$ is the $n$-degree characteristic polynomial $\det(U-\lambda I)=0$. The eigenvalues are the roots of it.
        \item $p(\lambda)=0$ if and only if $p(\bar{\lambda})=0$.
        \begin{itemize}
            \item $\lambda\in\C$ is an eigenvalue with eigenvector $\um\neq 0$ iff $U\um=\lambda\um$ and $U\bar{\um}=\bar{\lambda}\bar{\um}$.
        \end{itemize}
        \item Recall that $U$ unitary implies $|\lambda|=1$.
        \begin{itemize}
            \item Proof\footnote{This would be a good exam question.}: $\norm{U\x}=\norm{\x}$ and $U\x=\lambda\x$. Thus,
            \begin{equation*}
                \norm{U\x} = \norm{\lambda\x} = |\lambda|\norm{\x} = \norm{\x}
            \end{equation*}
            and since $\x\neq 0$, we can divide by $\norm{\x}$, so $|\lambda|=1$.
        \end{itemize}
        \item Let $\um=\Re\um+\Im\um$.
        \item It follows that we may define
        \begin{align*}
            \x &= \Re\um = \frac{\um+\bar{\um}}{2}&
            \y &= \Im\um = \frac{\um-\bar{\um}}{2}
        \end{align*}
        \item Thus, $\um=\x+i\y$ and $\bar{\um}=\x-i\y$.
        \item Since $U\x=\frac{U\um+U\bar{\um}}{2}=\frac{\lambda\um+\bar{\lambda}\bar{\um}}{2}$, $U\y=\Im(\lambda\um)=\Re(\lambda\um)$.
        \item Since $|\lambda|=1$, $\lambda=\e[i\alpha]$ and $\bar{\lambda}=\e[-i\alpha]$.
        \item It follows that $U\x=(\cos\alpha)\x-(\sin\alpha)\y$ and $U\y=(\cos\alpha)\y)+(\sin\alpha)\x$.
        \item Thus, since $U\x=\Re\lambda\um$, we have that
        \begin{align*}
            \lambda\um &= (\cos\alpha+i\sin\alpha)(\x+i\y)\\
            &= (\cos\alpha)\x-(\sin\alpha)\y+i[(\cos\alpha)\y+(\sin\alpha)\x]
        \end{align*}
        \item If $E_\lambda$ is a 2 dimensional space spanned by $\x$ and $\y$ and invariant by $U$. Thus, any block of the desired matrix leaves its desired sub-block invariant.
        \item We also know that the eigenvectors of a unitary matrix corresponding to different eigenvalues are orthogonal.
        \item Thus, $\norm{\x}=\norm{\y}=\sqrt{2}/2\norm{\um}$, $\x\perp\y$.
        \item Let $\x,\y$ complete the theorem to form a basis of $\R^n$.
        \item It will follow that
        \begin{equation*}
            U =
            \begin{pmatrix}
                R_\alpha & \bm{0}\\
                \bm{0} & U_1\\
            \end{pmatrix}
        \end{equation*}
        where $U_1$ is orthogonal, and we may repeat the process.
    \end{itemize}
\end{itemize}



\section{Chapter 6: Structure of Operators on Inner Product Spaces}
\emph{From \textcite{bib:Treil}.}
\begin{itemize}
    \item \marginnote{10/24:}Theorem 6.1.1: Let $A:X\to X$ be an operator acting in a complex inner product space. Then there exists an orthonormal basis $\um_1,\dots,\um_n$ of $X$ such that the matrix of $A$ in this basis is upper triangular. In other words, any $n\times n$ matrix $A$ can be represented as $A=UTU^*$, where $U$ is unitary and $T$ is upper-triangular.
    \item Theorem 6.1.2: Let $A:X\to X$ be an operator acting on a real inner product space. Suppose that all eigenvalues of $A$ are real. Then there exists an orthonormal basis $\um_1,\dots,\um_n$ in $X$ such that the matrix of $A$ in this basis is upper triangular. In other words, any real $n\times n$ matrix $A$ with all real eigenvalues can be represented as $T=UTU^*=UTU^T$, where $U$ is orthogonal and $T$ is a real upper-triangular matrix.
    \item Theorem 6.2.1: Let $A=A^*$ be a self-adjoint operator in an inner product space $X$ (the space can be complex or real). Then all eigenvalues of $A$ are real and there exists an orthonormal basis of eigenvectors of $A$ in $X$.\par
    Equivalently (see Theorem 6.2.2), $A$ can be represented as $A=UDU^*$ where $U$ is a unitary matrix and $D$ is a diagonal matrix with real entries. Moreover, if $A$ is real, $U$ can be chosen to be real, i.e., orthogonal.
    \item Proposition 6.2.3: Let $A=A^*$ be a self-adjoint operator and let $\lambda,\um,\mu,\vm$ be such that $A\um=\lambda\um$ and $A\vm=\mu\vm$. Then if $\lambda\neq\mu$, $\um\perp\vm$.
    \item Since complex multiplication is commutative,
    \begin{equation*}
        D^*D = DD^*
    \end{equation*}
    for every diagonal matrix $D$.
    \begin{itemize}
        \item It follows that $A^*A=AA^*$ if the matrix of $A$ in some orthonormal basis is diagonal.
    \end{itemize}
    \item Theorem 6.2.4: Any normal operator $N$ in a complex vector space has an orthonormal basis of eigenvectors.\par
    Equivalently, any matrix $N$ satisfying $N^*N=NN^*$ can be represented as $N=UDU^*$ where $U$ is unitary and $D$ is diagonal.
    \item Proposition 6.2.5: An operator $N:X\to X$ is normal iff
    \begin{equation*}
        \norm{N\x} = \norm{N^*\x}
    \end{equation*}
    for all $\x\in X$.
    \item \textbf{Hermitian square} (of $A$): The matrix $A^*A$.
    \item \textbf{Modulus} (of $A$): The unique positive semidefinite square root $\sqrt{A^*A}$.
    \item Proposition 6.3.3: For a linear operator $A:X\to Y$,
    \begin{equation*}
        \norm{|A|\x} = \norm{A\x}
    \end{equation*}
    \item Corollary 6.3.4: $\ker A=\ker|A|$.
    \item Theorem 6.3.5: Let $A:X\to X$ be an operator (square matrix). Then $A$ can be represented as
    \begin{equation*}
        A = U|A|
    \end{equation*}
    where $U$ is a unitary operator.
    \item \textbf{Singular value} (of $A$): An eigenvalue of $|A|$.
    \begin{itemize}
        \item A positive square root of an operator of $A^*A$.
    \end{itemize}
    \item Proposition 6.3.6: Let $\sigma_1,\dots,\sigma_n$ be the singular values of $A$, ordered such that $\sigma_1,\dots,\sigma_r$ are the nonzero singular values, and let $\vm_1,\dots,\vm_n$ be an orthonormal basis of eigenvectors of $A^*A$. Then the system
    \begin{equation*}
        \wm_k = \frac{1}{\sigma_k}A\vm_k
    \end{equation*}
    for $k=1,\dots,r$ is orthonormal.
    \item \textbf{Schmidt decomposition} (of $A$): The decompositions
    \begin{equation*}
        A = \sum_{k=1}^r\sigma_k\wm_k\vm_k^*
    \end{equation*}
    and
    \begin{equation*}
        A\x = \sum_{k=1}^r\sigma_k(\x,\vm_k)\wm_k
    \end{equation*}
    \begin{itemize}
        \item Note that these can be verified by plugging $\x=\vm_j$ for each $j=1,\dots,n$ into the latter equation.
    \end{itemize}
\end{itemize}




\end{document}