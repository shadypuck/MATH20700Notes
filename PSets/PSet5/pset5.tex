\documentclass[../psets.tex]{subfiles}

\pagestyle{main}
\renewcommand{\leftmark}{Problem Set \thesection}
\setcounter{section}{4}

\begin{document}




\section{Definiteness, Dual Spaces, and Advanced Spectral Theory}
\emph{From \textcite{bib:Treil}.}
\subsection*{Chapter 7}
\begin{enumerate}[label={\textbf{4.\arabic*.}}]
    \item \marginnote{11/1:}Using Silvester's Criterion of Positivity, check if the matrices
    \begin{align*}
        A &=
        \begin{pmatrix}
            4 & 2 & 1\\
            2 & 3 & -1\\
            1 & -1 & 2\\
        \end{pmatrix}&
        B &=
        \begin{pmatrix}
            3 & -1 & 2\\
            -1 & 4 & -2\\
            2 & -2 & 2\\
        \end{pmatrix}
    \end{align*}
    are positive definite or not. Are the matrices $-A$, $A^3$, $A^{-1}$, $A+B^{-1}$, $A+B$, and $A-B$ positive definite?
    \item True or false:
    \begin{enumerate}
        \item If $A$ is positive definite, then $A^5$ is positive definite.
        \item If $A$ is negative definite, then $A^8$ is negative definite.
        \item If $A$ is negative definite, then $A^{12}$ is positive definite.
        \item If $A$ is positive definite and $B$ is negative semidefinite, then $A-B$ is positive definite.
        \item If $A$ is indefinite, and $B$ is positive definite, then $A+B$ is indefinite.
    \end{enumerate}
    \item Let $A$ be a $2\times 2$ Hermitian matrix such that $a_{1,1}>0$, $\det A\geq 0$. Prove that $A$ is positive semidefinite.
    \item Find a real symmetric $n\times n$ matrix such that $\det A_k\geq 0$ for all $k=1,\dots,n$, but the matrix $A$ is not positive semidefinite. Try to find an example for the minimal possible $n$.
    \item Let $A$ be an $n\times n$ Hermitian matrix such that $\det A_k>0$ for all $k=1,\dots,n-1$ and $\det A\geq 0$. Prove that $A$ is positive semidefinite.
    \item Find a real symmetric $3\times 3$ matrix $A$ such that $a_{1,1}>0$, $\det A_k\geq 0$ for $k=2,3$, but the matrix $A$ is not positive semidefinite.
\end{enumerate}


\subsection*{Chapter 8}
\begin{enumerate}[label={\textbf{1.\arabic*.}}]
    \item Let $\vm_1,\dots,\vm_r$ be a system of vectors in $X$ such that there exists a system $\vm_1',\dots,\vm_r'$ of linear functionals such that
    \begin{equation*}
        \vm_k'(\vm_j) = \delta_{jk}
    \end{equation*}
    \begin{enumerate}
        \item Show that the system $\vm_1,\dots,\vm_r$ is linearly independent.
        \item Show that if the system $\vm_1,\dots,\vm_r$ is not generating, then the "biorthogonal" system $\vm_1',\dots,\vm_r'$ is not unique. (Hint: Probably the easiest way to prove that is to complete the system $\vm_1,\dots,\vm_r$ to a basis [see Proposition 2.5.4].)
    \end{enumerate}
\end{enumerate}

\begin{enumerate}[label={\textbf{3.\arabic*.}}]
    \item Prove that if for linear transformations $T,T_1:X\to Y$
    \begin{equation*}
        \langle T\x,\y'\rangle = \langle T_1\x,\y'\rangle
    \end{equation*}
    for all $x\in X$ and for all $\y'\in Y'$, then $T=T_1$. (Hint: Probably one of the easiest ways of proving this is to use Lemma 8.1.3.)
    \item Combine the Riesz Representation Theorem (Theorem 8.2.1) with the reasoning in Section 3.1.3 above to present a coordinate-free definition of the Hermitian adjoint of an operator in an inner product space.
    \item Let $\vm_1,\dots,\vm_n$ be a basis in $X$ and let $\vm_1',\dots,\vm_n'$ be its dual basis. Let $E=\spn\{\vm_1,\dots,\vm_r\}$ for $r<n$. Prove that $E^\perp=\spn\{\vm_{r+1}',\dots,\vm_n'\}$. (This problem gives a way to prove Proposition 8.3.6.)
\end{enumerate}


\subsection*{Chapter 9}
\begin{enumerate}[label={\textbf{1.\arabic*.}}]
    \item (Cayley-Hamilton Theorem for diagonalizable matrices). As discussed in Section 9.1, the Cayley-Hamilton theorem states that if $A$ is a square matrix and
    \begin{equation*}
        p(\lambda) = \det(A-\lambda I) = \sum_{k=0}^nc_k\lambda^k
    \end{equation*}
    is its characteristic polynomial, then $p(A)=\sum_{k=0}^nc_kA^k=\bm{0}$ (assuming that by definition, $A^0=I$). Prove this theorem for the special case when $A$ is similar to a diagonal matrix, i.e., $A=SDS^{-1}$. (Hint: If $D=\diag\{\lambda_1,\dots,\lambda_n\}$ and $p$ is any polynomial, can you compute $p(D)$? What about $p(A)$?)
\end{enumerate}

\begin{enumerate}[label={\textbf{2.\arabic*.}}]
    \item An operator $A$ is called \textbf{nilpotent} if $A^k=\bm{0}$ for some $k$. Prove that if $A$ is nilpotent, then $\sigma(A)=\{0\}$ (i.e., that 0 is the only eigenvalue of $A$). Can you do it without using the spectral mapping theorem?
\end{enumerate}




\end{document}